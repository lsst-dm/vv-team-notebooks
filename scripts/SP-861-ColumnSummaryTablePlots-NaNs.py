#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Authors:  Douglas Tucker, Gemini-2.5-Pro, Claude-Sonnet-3.5, Copilot GPT-5
# Created:  2025-10-15
# Modified:  2025-10-29
# Modified:  2025-10-30

"""
Description:
A command-line script to generate diagnostic plots for LSST table columns, focusing
on identifying issues by analyzing NaN (Not a Number) fractions. This script is a
conversion of the Jupyter Notebook 'SP-861-ColumnSummaryTablePlots-NaNs.ipynb'.

It can process data from two sources:
1.  A directory of local summary CSV files generated by a pipeline.
2.  A direct query to the Engineering Facility Database (EFD).

The script generates two main types of plots for each data filter (e.g., g, r, i):
1.  Detailed Plots: A set of histogram and scatter plots showing the distribution
    of NaN fractions for each numeric column in the table.
2.  Summary Plot: A single plot summarizing the 1st, 50th, and 99th percentiles
    of NaN fractions across all processed columns, providing a high-level overview.

All generated plots are saved as PNG files in the current working directory.

Usage Examples:
-----------------

# 1. Process data from local CSV files for LSSTCam, generating full and preview plots:
python process_table_data.py \\
    --instrument LSSTCam \\
    --collection "LSSTCam/runs/DRP/20250501_20250609/w_2025_26/DM-51580" \\
    --table-type objectTable \\
    --data-source csv \\
    --csv-dir "/path/to/your/SP-861/pipe_tasks/LSSTCam" \\
    --verbose 2 \\
    --preview

# 2. Process data by querying the EFD for LSSTComCam (verbose output):
python process_table_data.py \\
    --instrument LSSTComCam \\
    --collection "some/collection/name" \\
    --table-type sourceTable \\
    --data-source efd \\
    --efd-name "usdfdev_efd" \\
    --db-name "lsst.dm" \\
    --start-time "2025-04-08" \\
    --end-time "2025-12-31" \\
    --verbose 1
"""

# --- Preamble and Imports ---

# Set matplotlib backend to Agg to avoid GUI popups in a non-interactive environment
import matplotlib
matplotlib.use('Agg')

import os
import glob
import re
import argparse
import sys
import asyncio
import warnings
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from astropy.time import Time

# Conditionally import LSST-specific packages
try:
    from lsst_efd_client import EfdClient
    import lsst.daf.butler as dafButler
    from lsst.geom import SpherePoint, degrees
    from lsst.rsp import get_tap_service
    from astropy.coordinates import SkyCoord
    import astropy.units as u
    LSST_PACKAGES_AVAILABLE = True
except ImportError:
    LSST_PACKAGES_AVAILABLE = False
    print("Warning: Some LSST Science Pipeline packages are not installed (e.g., lsst_efd_client, lsst.daf.butler). EFD queries and dynamic tract lookups will not be available.", file=sys.stderr)


# --- Initial Setup ---

# Downgrade matplotlib INFO messages to WARNINGS to reduce log noise
mpl_logger = logging.getLogger('matplotlib')
mpl_logger.setLevel(logging.WARNING)

# Suppress common warnings for a cleaner output
warnings.filterwarnings("ignore")

# Configure pandas to display all rows when printing DataFrames
pd.set_option("display.max_rows", None)


# --- Function Definitions ---

def create_tract_markup_color_dict(repo, collection, skymap_name, top_n=20):
    """
    Queries LSSTCam science exposures from the Consolidated Database (ConsDB),
    determines tract IDs, and returns dictionaries for consistent plot styling.

    If LSST packages are unavailable or the query fails, it returns empty dicts
    and the script will fall back to a hardcoded default.
    """
    if not LSST_PACKAGES_AVAILABLE:
        print("Warning: LSST RSP packages not found. Cannot query ConsDB for tract info.", file=sys.stderr)
        return {}, {}, {}

    try:
        # --- Step 1: Query ConsDB ---
        cdb = get_tap_service("consdbtap")
        assert cdb is not None, "Could not connect to consdbtap service."

        query = """SELECT target_name, s_ra, s_dec
                   FROM cdb_lsstcam.visit1
                   WHERE img_type='science' AND can_see_sky='True'"""
        results = cdb.search(query).to_table()
        df_sci_exp = results.to_pandas()

        df_sci_exp['target_name'] = (
            df_sci_exp['target_name']
                .replace('', np.nan)
                .fillna('<blank>')
        )

        # --- Step 2: Load skymap ---
        butler = dafButler.Butler(repo, collections=collection)
        skyMap = butler.get('skyMap', skymap=skymap_name)

        # --- Step 3: Add tractId ---
        def get_tract_id(row):
            try:
                coord = SpherePoint(row['s_ra'], row['s_dec'], degrees)
                return skyMap.findTract(coord).getId()
            except Exception:
                return None
        df_sci_exp['tractId'] = df_sci_exp.apply(get_tract_id, axis=1)

        # --- Step 4: Top N targets ---
        top_targets = (
            df_sci_exp['target_name']
                .value_counts()
                .head(top_n)
                .index
                .tolist()
        )
        df_sci_exp_top = df_sci_exp[df_sci_exp['target_name'].isin(top_targets)]

        # --- Step 5: tract_dict_top ---
        counts = df_sci_exp_top.groupby(['tractId', 'target_name']).size().reset_index(name='count')
        tract_dict_top = (
            counts.sort_values(['tractId', 'count'], ascending=[True, False])
                  .drop_duplicates(subset='tractId')
                  .set_index('tractId')['target_name']
                  .to_dict()
        )

        # --- Step 6: Count unique tracts per target_name ---
        tract_counts = (
            df_sci_exp.groupby('target_name')['tractId']
                .nunique()
                .to_dict()
        )
        
        # --- Step 7: Sort top targets by tract count ---
        legend_order = sorted(top_targets, key=lambda t: tract_counts.get(t, 0), reverse=True)
        
        # --- Step 8: Assign markers/colors in sorted order ---
        marker_styles = ['o', 's', '^', 'v', '<', '>', 'D', 'p', '*', 'h', 'H', 'X', 'd', '|', '_', '+', 'x', 'P', '1', '2']
        color_cycle = plt.cm.tab20.colors
        
        marker_dict_top = {t: marker_styles[i % len(marker_styles)] for i, t in enumerate(legend_order)}
        color_dict_top = {t: color_cycle[i % len(color_cycle)] for i, t in enumerate(legend_order)}

        # Add "Other" category
        marker_dict_top["Other"] = '.'
        color_dict_top["Other"] = 'lightgray'

        return tract_dict_top, marker_dict_top, color_dict_top

    except Exception as e:
        print(f"Error querying ConsDB or processing tract data: {e}", file=sys.stderr)
        print("Falling back to a hardcoded dictionary for tract markup.", file=sys.stderr)
        return {}, {}, {}


def instrument_create_tract_markup_color_dict(instrument):
    """
    Provides hardcoded or dynamically generated dictionaries for tract styling
    based on the specified instrument.
    """
    if instrument == 'LSSTComCam':
        # From Slide 9 of https://docs.google.com/presentation/d/1NGzrT4t6wDGQ2-2a8rjioToquhx2vOP_KJTrPiCrDDY
        tract_dict={453: '47 Tuc', 454: '47 Tuc', 4849: 'ECDFS', 5063: 'ECDFS', 4848: 'ECDFS', 2394: 'EDFS', 2234: 'EDFS', 4016: 'Fornax', 4017: 'Fornax', 4218: 'Fornax', 4217: 'Fornax', 5525: 'Rubin_SV_095-25', 5526: 'Rubin_SV_095-25', 7611: 'Seagull', 7610: 'Seagull', 7850: 'Seagull', 10463: 'Rubin_SV_38_7', 10464: 'Rubin_SV_38_7', 10704: 'Rubin_SV_38_7'}
        marker_dict={'47 Tuc': 'o', 'ECDFS': 'v', 'EDFS': '^', 'Fornax': 's', 'Rubin_SV_095-25': '*', 'Seagull': 'd', 'Rubin_SV_38_7': 'P', 'Other': 'X'}
        color_dict= {'47 Tuc': 'b', 'ECDFS': 'g', 'EDFS': 'r', 'Fornax': 'c', 'Rubin_SV_095-25': 'm', 'Seagull': 'y', 'Rubin_SV_38_7': 'k', 'Other': 'gray'}
        return tract_dict, marker_dict, color_dict

    elif instrument == 'LSSTCam':
        # Attempt to get fresh data from the ConsDB
        tract_dict, marker_dict, color_dict = create_tract_markup_color_dict(
            repo='/repo/main',
            collection='LSSTCam/defaults',
            skymap_name='lsst_cells_v1'
        )
        # If the dynamic method fails, fall back to a hardcoded list
        if not tract_dict:
            print("Using outdated hardcoded list for LSSTCam tract markup.", file=sys.stderr)
            tract_dict={9813 : 'COSMOS', 1291 : 'Carina', 1292 : 'Carina', 1293 : 'Carina', 1417 : 'Carina', 9591 : 'DESI_SV3_R1', 9348 : 'DESI_SV3_R1', 10804 : 'M49', 10321 : 'M49', 10563 : 'M49', 10562 : 'M49', 10320 : 'M49', 10803 : 'M49', 6325: 'New_Horizons', 6324: 'New_Horizons', 6096: 'New_Horizons', 6323: 'New_Horizons', 6555: 'New_Horizons', 6554: 'New_Horizons', 3361: 'Prawn', 3360: 'Prawn', 3359: 'Prawn', 3362: 'Prawn', 3176: 'Prawn', 3175: 'Prawn', 8401:  'Rubin_SV_212_-7', 8400:  'Rubin_SV_212_-7', 8399:  'Rubin_SV_212_-7', 8641:  'Rubin_SV_212_-7', 8158:  'Rubin_SV_212_-7', 8883:  'Rubin_SV_212_-7', 6740:  'Rubin_SV_216_-17', 6973:  'Rubin_SV_216_-17', 3533:  'Rubin_SV_225_-40', 3534:  'Rubin_SV_225_-40', 3725:  'Rubin_SV_225_-40', 3346:  'Rubin_SV_225_-40', 3724:  'Rubin_SV_225_-40', 3345:  'Rubin_SV_225_-40', 3532:  'Rubin_SV_225_-40', 5857:  'Trifid-Lagoon', 5636:  'Trifid-Lagoon', 5635:  'Trifid-Lagoon', 5634:  'Trifid-Lagoon', 5858:  'Trifid-Lagoon', 1189:  'Alpha_Centauri', 1312:  'Alpha_Centauri', 1438:  'Alpha_Centauri', 1439:  'Alpha_Centauri_at_the_edge'}
            marker_dict={'COSMOS': 'o', 'Carina': 'v', 'DESI_SV3_R1': '^', 'M49': 's', 'New_Horizons': '*', 'Prawn': 'd', 'Rubin_SV_212_-7': 'P', 'Rubin_SV_216_-17': '<', 'Rubin_SV_225_-40': '>', 'Trifid-Lagoon': '8', 'Alpha_Centauri': 'h', 'Alpha_Centauri_at_the_edge': 'H', 'Other': 'X'}
            color_dict= {'COSMOS': 'b', 'Carina': 'g', 'DESI_SV3_R1': 'r', 'M49': 'c', 'New_Horizons': 'm', 'Prawn': 'pink', 'Rubin_SV_212_-7': 'k', 'Rubin_SV_216_-17': 'c', 'Rubin_SV_225_-40': 'aquamarine', 'Trifid-Lagoon': 'powderblue', 'Alpha_Centauri': 'violet', 'Alpha_Centauri_at_the_edge': 'darkviolet', 'Other': 'gray'}
        return tract_dict, marker_dict, color_dict
    else:
        return {}, {}, {}


def plot_hist_and_scatter(df, numeric_cols, filtername, table_type, collectionName, marker_dict, color_dict, super_title, save_path):
    """
    Creates and saves a figure with histogram and scatter plots for a list of numeric columns.
    Developed using ChatGPT-5 on Microsoft Copilot.
    """
    n_cols = len(numeric_cols)
    if n_cols == 0:
        return
    global_min, global_max = -0.1, 1.1

    # Auto-adjust figure height
    base_row_height = 4
    max_height = 250 if "PREVIEW" not in super_title else 40
    fig_height = min(base_row_height * n_cols, max_height)

    fig, axes = plt.subplots(n_cols, 2, figsize=(12, fig_height), sharex=False)
    plt.subplots_adjust(top=0.98, bottom=0.05, hspace=0.25, wspace=0.3)

    # Ensure axes is always 2D array for consistent indexing
    if n_cols == 1:
        axes = np.array([axes])

    # Loop over numeric columns
    for i, col in enumerate(numeric_cols):
        ax_hist = axes[i, 0]
        data = df[col].dropna()
        if data.empty:
            continue

        # Stats
        data_desc = data.describe(percentiles=[0.01, 0.99])
        mean, std = data_desc['mean'], data_desc['std']
        p01, median, p99 = data_desc['1%'], data_desc['50%'], data_desc['99%']

        # Histogram
        ax_hist.hist(data, bins=50, alpha=0.75, range=(global_min, global_max))
        xlabel = col.replace("_nan_fraction", "")
        if table_type == 'objectTable':
            xlabel = f"NaN Fraction in tract:    {xlabel}"
            ax_hist.set_ylabel('N_tracts')
        else: # sourceTable
            xlabel = f"NaN Fraction in visit:    {xlabel}"
            ax_hist.set_ylabel('N_visits')

        ax_hist.set_xlabel(xlabel)
        ax_hist.grid(True, alpha=0.3)
        ax_hist.axvline(mean, color='r', linestyle='--', alpha=0.8, label=f'Mean: {mean:.2f}')
        ax_hist.axvline(p01, color='b', linestyle='--', alpha=0.8, label=f'1%: {p01:.2f}')
        ax_hist.axvline(median, color='orange', linestyle='--', alpha=0.8, label=f'50%: {median:.2f}')
        ax_hist.axvline(p99, color='g', linestyle='--', alpha=0.8, label=f'99%: {p99:.2f}')
        ax_hist.text(0.10, 0.95, f'N: {len(data)}\nÏƒ: {std:.2f}', 
                     transform=ax_hist.transAxes, 
                     va='top', ha='right', 
                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        ax_hist.legend()
        ax_hist.set_xlim(global_min, global_max)

        # Scatter plot
        ax_scatter = axes[i, 1]
        for fieldname in df['fieldname'].unique():
            subset = df[df['fieldname'] == fieldname]
            # Apply same cut to scatter plot: drop any NaNs
            subset = subset[subset[col].notna()]
            if subset.empty:
                continue           
            marker = marker_dict.get(fieldname, 'x')
            color = color_dict.get(fieldname, 'gray')
            if table_type == 'objectTable':
                ax_scatter.scatter(subset[col], subset['tract'], marker=marker, color=color, label=fieldname, alpha=0.7)
                ax_scatter.set_ylabel('Tract')
            else: # sourceTable
                ax_scatter.scatter(subset[col], subset['visit'], marker=marker, color=color, label=fieldname, alpha=0.7)
                ax_scatter.set_ylabel('Visit')
            ax_scatter.set_xlim(global_min, global_max)
        ax_scatter.set_xlabel(xlabel)
        ax_scatter.grid(True, alpha=0.3)
        ax_scatter.legend(title='Fieldname', bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.suptitle(super_title, y=1.001, size=14, weight='bold')
    plt.tight_layout()

    fig.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"  > Generated plot with {n_cols} rows, saved to: {os.path.abspath(save_path)}")


def plot_nan_fraction_summary(df, table_type, collectionName, save_path, title_prefix="", reference_order=None, full_plot_path=None):
    """
    Plots and saves a summary scatterplot of NaN fraction percentiles.
    Developed using ChatGPT-5 on Microsoft Copilot.
    """
    # If a reference order is provided, reindex to match it (dropping missing)
    if reference_order is not None:
        df = df.loc[df.index.intersection(reference_order)]
        df = df.reindex(reference_order)

    n_rows = len(df)
    if n_rows == 0:
        return
        
    fig_height = max(4, n_rows * 0.25)
    fig = plt.figure(figsize=(10, fig_height))
    plt.grid(True)

    ax = sns.scatterplot(data=df, x="1%", y="name", color='b', label="1%")
    sns.scatterplot(data=df, x="50%", y="name", ax=ax, color='orange', label="50%")
    sns.scatterplot(data=df, x="99%", y="name", ax=ax, color='g', label="99%")
    plt.legend(title="Percentiles")
    plt.xlabel("NaN Fraction")

    # Build title
    if table_type == 'objectTable':
        title_text = f"{title_prefix}NaN Fraction in Tract\nCollection: {collectionName}"
    else: # sourceTable
        title_text = f"{title_prefix}NaN Fraction in Visit\nCollection: {collectionName}"

    if "PREVIEW" in title_prefix and full_plot_path:
        title_text += f"\nFull plot saved in {os.path.abspath(full_plot_path)}"

    plt.title(title_text)
    plt.margins(y=0.0075 if n_rows > 10 else 0.1)
    plt.tight_layout()

    fig.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"  > Generated summary plot with {n_rows} rows, saved to: {os.path.abspath(save_path)}")


async def get_data_from_efd(efd_name, db_name, table_type, time_lo, time_hi, verbose):
    """
    Connects to the EFD and retrieves data within a specified time range.
    """
    if not LSST_PACKAGES_AVAILABLE:
        print("Error: lsst_efd_client is not installed. Cannot query EFD.", file=sys.stderr)
        sys.exit(1)
        
    table_name = f"lsst.dm.{table_type}"
    query = f'''SELECT * FROM "{table_name}" WHERE time > '{time_lo}' and time < '{time_hi}' '''
    if verbose > 1:
        print(f"EFD Query:\n{query}")
    
    try:
        efd_client = EfdClient(efd_name, db_name=db_name)
        res = await efd_client.influx_client.query(query)
    except Exception as e:
        print(f"Error during EFD query: {e}", file=sys.stderr)
        sys.exit(1)

    if verbose > 0:
        print(f"Number of entries returned from EFD: {len(res)}")
        if not res.empty:
            key = 'tract' if table_type == 'objectTable' else 'visit'
            print(f"Unique {key}s returned: {len(res[key].unique())}")
    if verbose > 1 and not res.empty:
        print("--- EFD Query Result Head ---")
        print(res.head())
        
    return res


def load_data_from_csv(table_type, table_summary_dir, verbose):
    """
    Loads and combines data from multiple CSV summary files into a single DataFrame.
    """
    if table_type == 'objectTable':
        pattern = os.path.join(table_summary_dir, 'objectSummary.*.csv')
        id_col, id_regex = 'tract', re.compile(r'objectSummary\.(\d+)\.csv')
    elif table_type == 'sourceTable':
        pattern = os.path.join(table_summary_dir, 'sourceSummary.*.csv')
        id_col, id_regex = 'visit', re.compile(r'sourceSummary\.(\d+)\.csv')
    else:
        print(f"Error: Unrecognized table_type '{table_type}'", file=sys.stderr)
        sys.exit(1)

    files = glob.glob(pattern)
    if not files:
        print(f"Error: No CSV files found matching pattern '{pattern}'", file=sys.stderr)
        sys.exit(1)
        
    if verbose > 2:
        print("Found files:\n" + "\n".join(files))

    all_dfs = []
    for file_path in files:
        try:
            df_temp = pd.read_csv(file_path)
            filename = os.path.basename(file_path)
            match = id_regex.match(filename)
            if match:
                df_temp[id_col] = int(match.group(1))
            else:
                print(f"Warning: Could not extract {id_col} number from {filename}.", file=sys.stderr)
                df_temp[id_col] = pd.NA
            all_dfs.append(df_temp)
        except pd.errors.EmptyDataError:
            print(f"Warning: {file_path} is empty and will be skipped.", file=sys.stderr)
        except Exception as e:
            print(f"Error reading {file_path}: {e}", file=sys.stderr)

    if not all_dfs:
        print("No data loaded from CSV files.", file=sys.stderr)
        sys.exit(1)

    df = pd.concat(all_dfs, ignore_index=True)
    if 'Unnamed: 0' in df.columns:
        df = df.drop(columns=['Unnamed: 0'])
        
    if verbose > 1:
        print(f"Successfully loaded {df[id_col].nunique()} of {len(files)} CSV files.")
        print(f"Total rows in combined DataFrame: {len(df)}")
        print("--- Combined Raw DataFrame Head ---")
        print(df.head())

    # Pivot the table to get columns like 'ra_nan_fraction', 'dec_nan_fraction', etc.
    value_cols = ['percent_01', 'percent_50', 'percent_99', 'total_rows', 'nan_count', 'nan_fraction']
    df_piv = df.pivot_table(index=id_col, columns='colname', values=value_cols)
    df_piv.columns = df_piv.columns.swaplevel(0, 1)
    df_piv = df_piv.sort_index(axis=1)
    df_piv.columns = ['_'.join(col).strip() for col in df_piv.columns.values]
    res = df_piv.reset_index()
    
    if verbose > 1:
        print("\n--- Pivoted DataFrame Head ---")
        print(res.head())
        
    return res


# --- Main Execution ---

async def main(args):
    """
    Main function to orchestrate data loading, processing, and plotting.
    """
    # 1. Get plot styling dictionaries
    print(f"### 1. Preparing for instrument: {args.instrument}")
    tract_dict, marker_dict, color_dict = instrument_create_tract_markup_color_dict(args.instrument)

    # 2. Retrieve data
    print(f"\n### 2. Retrieving data from {args.data_source.upper()}")
    if args.data_source == 'csv':
        if not args.csv_dir:
            print("Error: --csv-dir must be specified for --data-source csv.", file=sys.stderr)
            sys.exit(1)
        res = load_data_from_csv(args.table_type, args.csv_dir, args.verbose)
    elif args.data_source == 'efd':
        res = await get_data_from_efd(args.efd_name, args.db_name, args.table_type, args.start_time, args.end_time, args.verbose)
    
    if res.empty:
        print("\nNo data returned. Exiting.", file=sys.stderr)
        return

    # 3. Prepare data for plotting
    print("\n### 3. Processing data and generating plots")
    id_col = 'tract' if args.table_type == 'objectTable' else 'visit'
    nan_fraction_columns = [col for col in res.select_dtypes(include='number').columns if col.endswith('_nan_fraction')]
    res_nan_fraction = res[[id_col] + nan_fraction_columns]

    df = res_nan_fraction.copy()
    if args.table_type == 'objectTable':
        df['fieldname'] = df['tract'].astype(int).map(tract_dict).fillna('Other')
    else:
        df['fieldname'] = 'Other' # No fieldname concept for visits in this script
    
    cols = df.columns.tolist()
    cols.insert(1, cols.pop(cols.index('fieldname')))
    df = df[cols]

    filternameList = ['u', 'g', 'r', 'i', 'z', 'y', 'none'] if args.table_type == 'objectTable' else ['all']
    cols_above_threshold = df.select_dtypes(include='number').columns[(df.select_dtypes(include='number') > 0.01).any()]
    summary_records = []
    
    # 4. Generate detailed plots for each filter
    for filtername in filternameList:
        print(f"\n--- Processing filter: {filtername} ---")

        if filtername in ('none', 'all'):
            numeric_cols = cols_above_threshold[~cols_above_threshold.str.match(r'^[ugrizy]_')]
        else:
            numeric_cols = cols_above_threshold[cols_above_threshold.str.match(f'^{filtername}_')]
        
        numeric_cols = numeric_cols.tolist()
        df_subset = df.copy() # Start with a full copy for each filter
        
        # --- Cleaning Step (for objectTable) ---
        dropped_count = 0
        dropped_ids = []
        if args.table_type == 'objectTable' and filtername not in ('none', 'all'):
            #check_cols = [c for c in numeric_cols if f"{filtername}_inputCount" not in c and f"{filtername}_psfModel_TwoGaussian_n_iter" not in c]
            check_cols = [f"{filtername}_ra_nan_fraction", f"{filtername}_dec_nan_fraction"]
            if check_cols:
                mask_check_cols = (df[check_cols] == 1.0).all(axis=1)
                dropped_count = mask_check_cols.sum()
                if dropped_count > 0:
                    dropped_ids = df.loc[mask_check_cols, 'tract'].tolist()
                    df_subset = df.loc[~mask_check_cols].copy()
                    print(f"  Dropped {dropped_count} tracts with all-NaN values for filter '{filtername}': {dropped_ids}")
        
        # Sort columns by median NaN fraction for plotting
        if numeric_cols:
            ## TEMPORARY KLUDGE!!!: Mask out values >= 1.0 so they don't affect the median
            #median_vals = df_subset[numeric_cols].mask(df_subset[numeric_cols] >= 1.0).median()
            median_vals = df_subset[numeric_cols].median()
            numeric_cols = median_vals.sort_values(ascending=False).index.tolist()
        else:
            print(f"  Skipping filter '{filtername}' (no numeric columns with NaN fraction > 0.01).")
            continue

        # Generate full plot
        superTitle = f"'NaN Fraction in {id_col.capitalize()}' for {args.table_type} Columns\nCollection: {args.collection}"
        outputPlotFile = f"NaN_Fraction_in_{id_col.capitalize()}_histograms_{filtername}_all.png"
        plot_hist_and_scatter(df_subset, numeric_cols, filtername, args.table_type, args.collection, marker_dict, color_dict, superTitle, outputPlotFile)

        # Generate paginated plots
        chunk_size = 3
        for i in range(0, len(numeric_cols), chunk_size):
            cols_chunk = numeric_cols[i:i+chunk_size]
            page_num = i // chunk_size + 1
            paginated_output_file = f"NaN_Fraction_in_{id_col.capitalize()}_histograms_{filtername}_page{page_num}.png"
            paginated_super_title = f"{superTitle}\n(Page {page_num})"
            plot_hist_and_scatter(df_subset, cols_chunk, filtername, args.table_type, args.collection, marker_dict, color_dict, paginated_super_title, paginated_output_file)

        # Generate preview plot if requested
        if args.preview:
            preview_outputFile = f"NaN_Fraction_in_{id_col.capitalize()}_histograms_{filtername}_PREVIEW.png"
            preview_superTitle = f"PREVIEW (Top 20 Columns, filter={filtername}):\n{superTitle}"
            plot_hist_and_scatter(df_subset, numeric_cols[:20], filtername, args.table_type, args.collection, marker_dict, color_dict, preview_superTitle, preview_outputFile)
        
        summary_records.append({"Filter": filtername, "Columns Plotted": len(numeric_cols), f"{id_col.capitalize()}s Dropped": dropped_count})

    print("\n" + pd.DataFrame(summary_records).to_string(index=False))

    # 5. Generate summary plots
    print("\n--- Generating Summary Plots ---")
    ## TEMPORARY KLUDGE!!!: remove tract+column combos with NaN_Fraction >= 1.0
    #df_clean = res_nan_fraction.mask(res_nan_fraction >= 1.0)
    df_clean = res_nan_fraction.copy()
    summary_df = df_clean.drop(columns=[id_col]).describe(percentiles=[0.01,0.99]).T
    summary_df['name'] = summary_df.index.str.replace('_nan_fraction', '', regex=False)
    
    for f in filternameList:
        if f in ('none', 'all'):
            mask = ~summary_df['name'].str.match(r'^[ugrizy]_')
        else:
            mask = summary_df['name'].str.startswith(f"{f}_")
        
        df_subset = summary_df.loc[mask].copy()
        if df_subset.empty:
            continue

        outputPlotFile = f'NaN_Fraction_in_{id_col.capitalize()}_{f}_percentiles.png'
        superTitle = f"'NaN Fraction in {id_col.capitalize()}' for {args.table_type} Columns (filter={f})\nCollection: {args.collection}"

        # Full summary plot
        plot_nan_fraction_summary(df_subset, args.table_type, args.collection, outputPlotFile, title_prefix="")

        # Preview summary plot
        if args.preview:
            preview_outputPlotFile = f'NaN_Fraction_in_{id_col.capitalize()}_{f}_percentiles_PREVIEW.png'
            plot_nan_fraction_summary(df_subset.head(20), args.table_type, args.collection, preview_outputPlotFile, title_prefix="PREVIEW (First 20 Columns): ", full_plot_path=outputPlotFile)

    # 6. Clean up
    pd.reset_option("display.max_rows")
    print("\n### Script finished successfully. ###")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate diagnostic plots for LSST table columns.", formatter_class=argparse.RawTextHelpFormatter)
    
    # General arguments
    parser.add_argument('-i', '--instrument', choices=['LSSTCam', 'LSSTComCam'], required=True, help='Instrument name.')
    parser.add_argument('-c', '--collection', type=str, required=True, help='Butler collection name for context and titles.')
    parser.add_argument('-t', '--table-type', choices=['objectTable', 'sourceTable'], required=True, help='The type of table to process.')
    parser.add_argument('-v', '--verbose', action='count', default=0, help='Increase output verbosity. Can be used multiple times (e.g., -vv).')
    parser.add_argument('--preview', action='store_true', help='Generate smaller preview plots for the top 20 columns.')
    
    # Data source arguments
    parser.add_argument('--data-source', choices=['csv', 'efd'], required=True, help='Specify the source of the data.')
    
    # CSV-specific arguments
    csv_group = parser.add_argument_group('CSV Data Source Options')
    csv_group.add_argument('--csv-dir', type=str, help='Directory path containing the summary CSV files (required for --data-source csv).')
    
    # EFD-specific arguments
    efd_group = parser.add_argument_group('EFD Data Source Options')
    efd_group.add_argument('--efd-name', type=str, default='usdfdev_efd', help='Name of the EFD instance.')
    efd_group.add_argument('--db-name', type=str, default='lsst.dm', help='Name of the database within the EFD.')
    efd_group.add_argument('--start-time', type=str, default='2025-04-08', help='Start time for EFD query (YYYY-MM-DD).')
    efd_group.add_argument('--end-time', type=str, default='2025-12-31', help='End time for EFD query (YYYY-MM-DD).')
    
    args = parser.parse_args()
    
    # Run the main async function
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\nScript interrupted by user.")
        sys.exit(0)
