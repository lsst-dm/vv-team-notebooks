{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79f6019-6d93-4ae3-b977-e871420921eb",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250 style=\"padding: 10px\"> \n",
    "<br><b>Prototype Target Dashboard</b> <br>\n",
    "Contact author: Douglas Tucker <br>\n",
    "Last verified to run: 2024-10-10 <br>\n",
    "LSST Science Pipelines version: Weekly 2024_37 <br>\n",
    "Container size: medium <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90fdc1-462a-425e-948c-5ca98a52f1e1",
   "metadata": {},
   "source": [
    "**Description:** A prototype dashboard for exploring targets (and science programs) for a given night.  This version focuses on AuxTel observations, but should be also useful for ComCom and LSSTcam commissioning, when there will be a greater diversity of observational programs than those provided by the survey scheduler during standard operations.\n",
    "\n",
    "See Jira issue <a href=https://rubinobs.atlassian.net/browse/SP-924>SP-924</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cbb2d-467b-4ddc-87df-635862976392",
   "metadata": {},
   "source": [
    "**Credit:** Based heavily on the <a href=https://github.com/sylvielsstfr/AuxTelComm/blob/main/notebooks_usdf/ana_auxtelprod_jn/spectractor_v3.1.0_May2024/ListOfExposures-hologram-oga.ipynb> List of Exposures at USDF in OGA</a> notebook by Sylvie Dagoret-Campagne.  This initial \"Prototype Target Dashboard\" notebook would not have been possible without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8dfcc-d745-4fc3-ad39-6b2d5e0e2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1. Preliminaries\n",
    "#\n",
    "### 1.1 Parameter input\n",
    "#\n",
    "### LSSTComSamSim for OR4:\n",
    "##instrument = 'LSSTComCamSim'\n",
    "##repo = 'embargo_or4'\n",
    "##day_obs = '2024-06-25'\n",
    "##collection = ['LSSTComCamSim/prompt/output-2024-06-25',\n",
    "##              'LSSTComCamSim/prompt/output-2024-06-26',\n",
    "##              'LSSTComCamSim/prompt/output-2024-06-27']\n",
    "##collection_sky = ['LSSTComCamSim/runs/nightlyValidation/20240625/w_2024_25/DM-44966',\n",
    "##                  'LSSTComCamSim/runs/nightlyValidation/20240626/w_2024_25/DM-44966',\n",
    "##                  'LSSTComCamSim/runs/nightlyValidation/20240627/w_2024_25/DM-44966']\n",
    "##skymap_name = 'ops_rehersal_prep_2k_v1'\n",
    "##\n",
    "##col_sciprog = 'science_program'\n",
    "###col_target = 'target'\n",
    "###col_target = 'HPX8'  # HPX8 is used as a proxy for target_name or block\n",
    "##col_target = 'field_name'  # field_name is used as a proxy for target_name or block\n",
    "##col_filter = 'filter'\n",
    "##col_id = 'id'\n",
    "#\n",
    "#\n",
    "## LATISS:\n",
    "#instrument = 'LATISS'\n",
    "#repo = '/repo/embargo'\n",
    "#day_obs = '2024-08-07'\n",
    "#collection = ['LATISS/prompt/output-2024-08-07']\n",
    "#collection_sky = ['LATISS/runs/AUXTEL_DRP_IMAGING_20230509_20240513/w_2024_20/PREOPS-5146']\n",
    "#skymap_name = 'latiss_v1'\n",
    "#\n",
    "#col_sciprog = 'science_program'\n",
    "#col_target = 'target'\n",
    "##col_target = 'HPX8'  # HPX8 is used as a proxy for target_name or block\n",
    "##col_target = 'field_name'  # field_name is used as a proxy for target_name or block\n",
    "#col_filter = 'filter'\n",
    "#col_id = 'id'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecc10d-2938-440e-8d5f-84dbfde6cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert the string to a datetime object\n",
    "day_obs_obj = datetime.strptime(day_obs, '%Y-%m-%d')\n",
    "\n",
    "# Format the datetime object as an integer\n",
    "day_obs_int = int(day_obs_obj.strftime('%Y%m%d'))\n",
    "\n",
    "print(day_obs_int)  # Output: 20240807\n",
    "\n",
    "day_obs = day_obs_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c458d63-e50a-42a2-82d1-9b2c2fbb4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set instrument, day_obs, and (sometimes) pipeline:\n",
    "\n",
    "# LSSTComSamSim for OR4:\n",
    "#instrument = 'LSSTComCamSim'\n",
    "#day_obs = 20240625\n",
    "##pipeline = 'nv' # either 'nv' (nightly validation), 'pp' (prompt processing), or 'drp' (data release pipeline)\n",
    "#pipeline = 'pp' # either 'nv' (nightly validation), 'pp' (prompt processing), or 'drp' (data release pipeline)\n",
    "##pipeline = 'drp' # either 'nv' (nightly validation), 'pp' (prompt processing), or 'drp' (data release pipeline)\n",
    "\n",
    "\n",
    "## LATISS:\n",
    "#instrument = 'LATISS'\n",
    "#day_obs = 20240807\n",
    "\n",
    "#pipeline = 'pp' # either 'pp' (prompt processing) or 'drp' (data release pipeline)\n",
    "##pipeline = 'drp' # either 'pp' (prompt processing) or 'drp' (data release pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88436628-5abd-470a-be6f-80e9b1d9a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set repo, collection, and columns to use based on instrument, day_obs, and (sometimes) pipeline:\n",
    "#\n",
    "#if instrument == 'LSSTComCamSim':\n",
    "#\n",
    "#    # From Meredith Rawls:\n",
    "#    repo = 'embargo_or4'\n",
    "#    instrument = 'LSSTComCamSim'\n",
    "#    skymap_name = 'ops_rehersal_prep_2k_v1'\n",
    "#    \n",
    "#    nv_collections = ['LSSTComCamSim/runs/nightlyValidation/20240625/w_2024_25/DM-44966',\n",
    "#                      'LSSTComCamSim/runs/nightlyValidation/20240626/w_2024_25/DM-44966',\n",
    "#                      'LSSTComCamSim/runs/nightlyValidation/20240627/w_2024_25/DM-44966']\n",
    "#    \n",
    "#    drp_collections = 'LSSTComCamSim/runs/DRP/OR4/w_2024_25/DM-45066'\n",
    "#    \n",
    "#    pp_collections = ['LSSTComCamSim/prompt/output-2024-06-25',\n",
    "#                      'LSSTComCamSim/prompt/output-2024-06-26',\n",
    "#                      'LSSTComCamSim/prompt/output-2024-06-27']\n",
    "#    \n",
    "#    if pipeline == 'drp':\n",
    "#        collection=drp_collections\n",
    "#        collection_sky=collection\n",
    "#    elif pipeline == 'nv':\n",
    "#        collection=nv_collections\n",
    "#        collection_sky=nv_collections\n",
    "#    elif pipeline == 'pp': \n",
    "#        collection=pp_collections\n",
    "#        # prompt processing does not seem to have its own sky map?\n",
    "#        #  use the nv_collections skymap instead?\n",
    "#        collection_sky=nv_collections\n",
    "#    else:\n",
    "#        print('ERROR:  unrecognized pipeline.  Stopping here...')\n",
    "#        raise StopExecution\n",
    "#\n",
    "#    col_sciprog = 'science_program'\n",
    "#    #col_target = 'target'\n",
    "#    #col_target = 'HPX8'  # HPX8 is used as a proxy for target_name or block\n",
    "#    col_target = 'field_name'  # field_name is used as a proxy for target_name or block\n",
    "#    col_filter = 'filter'\n",
    "#    col_id = 'id'\n",
    "#\n",
    "#elif instrument == 'LATISS':\n",
    "#\n",
    "#    skymap_name = 'latiss_v1'\n",
    "#\n",
    "## From Huan Lin:\n",
    "#    drp_repo = 'embargo_or4'\n",
    "#    drp_collections = ['LATISS/runs/AUXTEL_DRP_IMAGING_20230509_20240807/w_2024_30/PREOPS-5352']\n",
    "#    drp_collections_sky = drp_collections\n",
    "#\n",
    "#    pp_repo = '/repo/embargo'\n",
    "#    pp_collections = ['LATISS/prompt/output-2024-08-07']\n",
    "#    pp_collections_sky = ['LATISS/runs/AUXTEL_DRP_IMAGING_20230509_20240513/w_2024_20/PREOPS-5146']\n",
    "#    \n",
    "#    if pipeline == 'drp':\n",
    "#        repo=drp_repo\n",
    "#        collection=drp_collections\n",
    "#        collection_sky=collection\n",
    "#    elif pipeline == 'pp': \n",
    "#        repo=pp_repo\n",
    "#        collection=pp_collections\n",
    "#        # prompt processing does not seem to have its own sky map?\n",
    "#        collection_sky=pp_collections_sky\n",
    "#    else:\n",
    "#        print('ERROR:  unrecognized pipeline.  Stopping here...')\n",
    "#        raise StopExecution\n",
    "#\n",
    "#    col_sciprog = 'science_program'\n",
    "#    col_target = 'target'\n",
    "#    #col_target = 'HPX8'  # HPX8 is used as a proxy for target_name or block\n",
    "#    #col_target = 'field_name'  # field_name is used as a proxy for target_name or block\n",
    "#    col_filter = 'filter'\n",
    "#    col_id = 'id'\n",
    "# \n",
    "#else:\n",
    "#\n",
    "#    print('Unsupported instrument.  Stopping here...')\n",
    "#    raise StopExecution\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e874e-4cba-4e91-b6e3-b6dc6ea1bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3556a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "from matplotlib.colors import LogNorm\n",
    "from IPython.display import display, HTML\n",
    "import lsst.afw.display as afwDisplay\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.geom as geom\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.ticker\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0c5be-68cf-4151-9718-0c2c581cc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1.3 Define functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7808f-8111-4c23-8735-5a8fbc26b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a function to create a pandas DataFrame, df_grouped, that contains the results of performing a 2D groupby \n",
    "# operation on a pandas DataFrame, df, and counts the number of entries in the col_count column for each (col_x, col_y).  \n",
    "# One can choose to include a count of zero for cases where (col_x, col_y) have no entries in col_count (the default), \n",
    "# or to exclude those cases.  The resulting pandas DataFrame, df_grouped, is also sorted by (col_x, col_y).\n",
    "\n",
    "# (Thanks to the ChatGPT on Poe.com and to https://stackoverflow.com/questions/37003100/pandas-groupby-for-zero-values \n",
    "# for help in creating this function!)\n",
    "\n",
    "def df2Dgroupby(df, col_x, col_y, col_count, fill_zeros=True):\n",
    "\n",
    "    if fill_zeros==True:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_x, col_y])[col_count]\n",
    "            .count()\n",
    "            .unstack(fill_value=0)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={0: \"number\"})\n",
    "            .sort_values([col_x, col_y])\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_x, col_y])[col_count]\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .rename(columns={col_count: \"number\"})\n",
    "            .sort_values([col_x, col_y])\n",
    "        )\n",
    "        \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48629971-c159-4e22-af48-f1a7afb61839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The same, but for performing the `groupby` over 3 columns. \n",
    "\n",
    "# (Thanks to the ChatGPT on Poe.com and to https://stackoverflow.com/questions/37003100/pandas-groupby-for-zero-values \n",
    "# for help in creating this function!)\n",
    "\n",
    "def df3Dgroupby(df, col_x, col_y, col_z, col_count, fill_zeros=True):\n",
    "\n",
    "    if fill_zeros==True:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_x, col_y, col_z])[col_count]\n",
    "            .count()\n",
    "            .unstack(fill_value=0)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={0: \"number\"})\n",
    "            .sort_values([col_x, col_y, col_z])\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_x, col_y, col_z])[col_count]\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .rename(columns={col_count: \"number\"})\n",
    "            .sort_values([col_x, col_y, col_z])\n",
    "        )\n",
    "        \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfadfc-7a02-4634-a264-cfb4c0e40762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The same, but for performing the `groupby` over 4 columns. \n",
    "\n",
    "# (Thanks to the ChatGPT on Poe.com and to https://stackoverflow.com/questions/37003100/pandas-groupby-for-zero-values \n",
    "# for help in creating this function!)\n",
    "\n",
    "def df4Dgroupby(df, col_w, col_x, col_y, col_z, col_count, fill_zeros=True):\n",
    "\n",
    "    if fill_zeros==True:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_w, col_x, col_y, col_z])[col_count]\n",
    "            .count()\n",
    "            .unstack(fill_value=0)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={0: \"number\"})\n",
    "            .sort_values([col_w, col_x, col_y, col_z])\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_grouped = (\n",
    "            df.groupby([col_w, col_x, col_y, col_z])[col_count]\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .rename(columns={col_count: \"number\"})\n",
    "            .sort_values([col_w, col_x, col_y, col_z])\n",
    "        )\n",
    "        \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef46fb-8f50-49d0-bbae-0a2662baa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a function to perform a many-to-one match based on RA, DEC sky coordinates:\n",
    "# (Kudos to Claude-3.5-Sonnet AI from Poe.com for this function!)\n",
    "\n",
    "def skyMatchCatManytoOne(df_a, df_a_ra_colname, df_a_dec_colname, df_b, df_b_ra_colname, df_b_dec_colname, max_sep_deg, verboseFlag=False):\n",
    "\n",
    "    from astropy.coordinates import SkyCoord # High-level coordinates\n",
    "    from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n",
    "    from astropy.coordinates import Angle, Latitude, Longitude  # Angles\n",
    "    from astropy import units as u\n",
    "    from astropy.coordinates.matching import search_around_sky\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create SkyCoord objects for both DataFrames\n",
    "    coords_a = SkyCoord(ra=df_a[df_a_ra_colname].values*u.degree, dec=df_a[df_a_dec_colname].values*u.degree)\n",
    "    coords_b = SkyCoord(ra=df_b[df_b_ra_colname].values*u.degree, dec=df_b[df_b_dec_colname].values*u.degree)\n",
    "\n",
    "    # Set a maximum separation for matching (adjust as needed)\n",
    "    max_sep_deg = max_sep_deg * u.degree\n",
    "\n",
    "    # Perform the search\n",
    "    idx_a, idx_b, d2d, _ = search_around_sky(coords_a, coords_b, max_sep_deg)\n",
    "\n",
    "    # Create a DataFrame with the matches\n",
    "    df_matches = pd.DataFrame({\n",
    "        'idx_a': idx_a,\n",
    "        'idx_b': idx_b,\n",
    "        'separation': d2d.degree\n",
    "    })\n",
    "\n",
    "    # Group by idx_a and find the minimum separation for each\n",
    "    best_matches = df_matches.loc[df_matches.groupby('idx_a')['separation'].idxmin()]\n",
    "\n",
    "    # Create the final matched DataFrame, keeping all entries from df_a\n",
    "    df_matched = df_a.copy()\n",
    "\n",
    "    # Prepare the data from df_b to be joined\n",
    "    df_b_matched = df_b.iloc[best_matches['idx_b']].copy()\n",
    "    df_b_matched['idx_a'] = best_matches['idx_a'].values\n",
    "    df_b_matched['separation'] = best_matches['separation'].values\n",
    "    df_b_matched = df_b_matched.set_index('idx_a')\n",
    "\n",
    "    # Perform the left join\n",
    "    df_matched = df_matched.join(df_b_matched, rsuffix='_b')\n",
    "    \n",
    "    # Reset index if needed\n",
    "    df_matched = df_matched.reset_index(drop=True)\n",
    "\n",
    "    # Fill NaN values in the 'separation' column with a sentinel value (e.g., -1) to indicate no match\n",
    "    df_matched['separation'] = df_matched['separation'].fillna(-1)\n",
    "\n",
    "    # Optionally, you can add a boolean column to easily identify matched and unmatched rows\n",
    "    df_matched['has_match'] = df_matched['separation'] != -1\n",
    "\n",
    "    if verboseFlag:\n",
    "        # Print some statistics\n",
    "        print(f\"Total entries in df_a: {len(df_a)}\")\n",
    "        print(f\"Entries in df_a with matches: {df_matched['has_match'].sum()}\")\n",
    "        print(f\"Entries in df_a without matches: {(~df_matched['has_match']).sum()}\")\n",
    "\n",
    "    return df_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b41be8-c1e3-428c-9aa2-ab6e2f46afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define some healpix tools <span style=\"color:red; font-weight:bold\">(Maybe no longer necessary?)</span>:\n",
    "# (Thanks to Sahar Allam and Alex Drlica-Wagner!)\n",
    "\n",
    "##################################\n",
    "def radec2thetaphi(ra, dec):\n",
    "    import numpy as np\n",
    "    return (90-dec)*np.pi/180., ra*np.pi/180.\n",
    "\n",
    "##################################\n",
    "#DESDM uses nside=128, nest=True\n",
    "#Alex Drlica Wagner's healpixelated Gaia DR2 on des40 uses nside=32, nest=False\n",
    "def getipix(nside,ra,dec,nest=True):\n",
    "    import healpy as hp\n",
    "    theta, phi = radec2thetaphi(ra, dec)\n",
    "    ipix = hp.pixelfunc.ang2pix(nside, theta, phi, nest)\n",
    "    return ipix\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06739927-d21e-449c-ac6a-89e0118d14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to return the tract id, given the ra, dec and a skymap:\n",
    "\n",
    "def find_tract(ra,dec,skymap):\n",
    "\n",
    "    import lsst.geom as geom\n",
    "    \n",
    "    try:\n",
    "        sky_point = geom.SpherePoint(ra * geom.degrees,\n",
    "                                     dec * geom.degrees)\n",
    "\n",
    "        tract = skymap.findTract(sky_point)\n",
    "        tract = tract.tract_id\n",
    "    \n",
    "        del sky_point\n",
    "\n",
    "    except:\n",
    "        \n",
    "        tract = -9999\n",
    "    \n",
    "    return tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff24f55-6d1c-4f18-9f60-99431c2a5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use queryDataIds to find tracts that have overlap with an exposure.\n",
    "#  (Via Krzysz Findeisen and Meredith Rawls!  Plus a little help from the Poe.com AI Assistant.)\n",
    "\n",
    "def find_tracts_overlapping_exposure(exposure_id, instrument, butler):\n",
    "    \n",
    "    tract_set = set(list(butler.registry.queryDataIds(dimensions=('visit', 'tract'), \n",
    "                                                      instrument=instrument, \n",
    "                                                      exposure=exposure_id)))\n",
    "    \n",
    "    tract_values = [item['tract'] for item in tract_set]\n",
    "    tract_values = list(set(tract_values))\n",
    "    # Note:  tract_values.sort() sorts in place and returns `None`!\n",
    "    #  So either use `tract_values.sort()` or `tract_values = sorted(tract_values)`\n",
    "    tract_values = sorted(tract_values)\n",
    "    \n",
    "    return tract_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b16bb4-cd15-402f-81c2-7b7193df0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Return a string of the list of tracts overlapping the exposure.\n",
    "#  If the function find_tracts_overlapping_exposure does not find a list,\n",
    "#  use find_tract to find the tract nearest the center of the exposure.\n",
    "#  If find_tract fails, it will return -9999.\n",
    "#  Append an asterisk (*) to the tract list if it is provided by find_tract.\n",
    "\n",
    "def return_string_of_tracts_overlapping_exposure_plus(exposure_id, instrument, butler, skymap, ra, dec):\n",
    "\n",
    "    list_of_tracts = find_tracts_overlapping_exposure(exposure_id, instrument, butler)\n",
    "    \n",
    "    if not list_of_tracts:\n",
    "        central_tract = find_tract(ra,dec,skymap)\n",
    "        list_of_tracts_string = str(central_tract)+'*'\n",
    "    else:\n",
    "        list_of_tracts_string = ','.join(map(str, list_of_tracts))\n",
    "\n",
    "    #print(exposure_id, list_of_tracts_string)\n",
    "\n",
    "    return list_of_tracts_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c9f13-1b9a-4d60-84a8-50368873fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a class to stop \"Run All\" at a code cell containing the command \"raise StopExecution\":\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ba6a7-c940-4c0c-a513-f9a4a1dab1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set filter warnings to \"ignore\" to avoid a lot of \"logorrhea\" to the screen:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07168a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set default backend for afwDisplay to matplotlib:\n",
    "afwDisplay.setDefaultBackend('matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff8f23-c35c-4b72-b9f5-7d32b432ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set a few parameters to use later, when plotting:\n",
    "\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "plot_filter_labels = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "plot_filter_colors = {'u': '#56b4e9', 'g': '#008060', 'r': '#ff4000',\n",
    "                      'i': '#850000', 'z': '#6600cc', 'y': '#000000'}\n",
    "plot_filter_symbols = {'u': 'o', 'g': '^', 'r': 'v', 'i': 's', 'z': '*', 'y': 'p'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a006c6f-b6c6-4dc6-af86-40be86e2a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 1.4 Add Ops Rehearsal 4 field table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9ef6c-ced2-4d09-b67a-419f1b33d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a temporary fix to attach field names to the Ops Rehearsal 4 science exposures:\n",
    "# Table of Ops Rehearsal 4 Fields from https://confluence.lsstcorp.org/pages/viewpage.action?pageId=259785406\n",
    "ascii_table = \"\"\"\n",
    "Field                       RA          DEC     stellarDens     E_BV\n",
    "Rubin_SV_095_-25           95.00      -25.00        2.92        0.05\n",
    "Rubin_SV_125_-15          125.00      -15.00        3.78        0.06\n",
    "DESI_SV3_R1               179.60        0.00        0.53        0.03\n",
    "Rubin_SV_225_-40          225.00      -40.00       12.67        0.08\n",
    "DEEP_A0                   216.00      -12.50        1.17        0.09\n",
    "Rubin_SV_250_2            250.00        2.00        4.56        0.07\n",
    "Rubin_SV_300_-41          300.00      -41.00        5.22        0.09\n",
    "Rubin_SV_280_-48          280.00      -48.00       17.64        0.07\n",
    "DEEP_B0                   310.00      -19.00        3.28        0.04\n",
    "ELAIS_S1                    9.45      -44.00        0.56        0.01\n",
    "XMM_LSS                    35.71       -4.75        0.47        0.03\n",
    "ECDFS                      53.12      -28.10        0.61        0.01\n",
    "COSMOS                    150.10        2.18        0.67        0.02\n",
    "EDFS_A                     58.90      -49.31        0.75        0.01\n",
    "EDFS_B                     63.60      -47.60        0.81        0.01\"\"\"\n",
    "\n",
    "# Convert the string to a file-like object\n",
    "ascii_io = io.StringIO(ascii_table)\n",
    "\n",
    "# Read the ASCII table into a pandas DataFrame\n",
    "df_field = pd.read_csv(ascii_io, delim_whitespace=True)\n",
    "\n",
    "# Add healpix columns to this table, one very roughly the area of a \n",
    "# ComCam field-of-view (NSIDE=2^6=64 --> 0.84 sq deg) and one very roughly \n",
    "# the area of an LSSTCam field-of-view (NSIDE=2^4=16 --> 13 sq deg) \n",
    "# ***(Maybe no longer necessary?)***\n",
    "df_field.loc[:,'HPX64'] = getipix(64, df_field.loc[:,'RA'].values, df_field.loc[:,'DEC'].values)\n",
    "df_field.loc[:,'HPX16'] = getipix(16, df_field.loc[:,'RA'].values, df_field.loc[:,'DEC'].values)\n",
    "\n",
    "## Display df_field\n",
    "#df_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335850aa-93e7-4ea4-a4ef-26c67d2ef960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Access data for this repo, collection, and day of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f6893-2386-44b4-ae20-9206519c4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2.1 Instantiate butler and create registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81254df0-8ec8-4515-b2dc-743f3cceca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "butler = dafButler.Butler(repo, collections=collection)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac6040-cd29-44ac-bdbe-23d29bf91ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2.2 Read in information from the `exposure` dimension and create pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9988a3-81be-4bf4-9c42-5be4da1aced2",
   "metadata": {},
   "source": [
    "## Total Number of Exposures Returned from Butler Query for the Instrument and Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d36237-32a9-486b-893a-646ab892ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query the metadata for the `exposure` dimension, limiting the results to this particular instrument and day of observation:\n",
    "query=\"instrument='%s' AND day_obs=%d\" % (instrument, day_obs)\n",
    "results = registry.queryDimensionRecords('exposure',where=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d8db3-ca82-450e-bfd9-29bbdf26cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop executing if there are no results returned:\n",
    "\n",
    "n_results = results.count()\n",
    "\n",
    "if n_results <= 0:\n",
    "    raise StopExecution\n",
    "else:\n",
    "    print(\"\"\"There are %d results returned from querying the butler for instrument %s on %d.\"\"\" % (n_results, instrument, day_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cdd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate a pandas `DataFrame` with useful columns available in the `exposure` dimension:\n",
    "df_exp = pd.DataFrame(columns=['id', 'obs_id','day_obs', 'seq_num',\n",
    "                                    'time_start','time_end' ,'type', 'reason', \n",
    "                                    'target','filter','zenith_angle',\n",
    "                                    'expos','ra','dec','skyangle',\n",
    "                                    'azimuth','zenith','science_program',\n",
    "                                    'jd','mjd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9f07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the query results into the new pandas `DataFrame`:\n",
    "\n",
    "for count, info in enumerate(results):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df_exp.loc[count] = [info.id, info.obs_id, info.day_obs, info.seq_num, \n",
    "                                  pd.to_datetime(info.timespan.begin.to_string()),\n",
    "                                  pd.to_datetime(info.timespan.end.to_string()),\n",
    "                                  info.observation_type, info.observation_reason, info.target_name, \n",
    "                                  info.physical_filter, info.zenith_angle, \n",
    "                                  info.exposure_time,info.tracking_ra, info.tracking_dec, \n",
    "                                  info.sky_angle,info.azimuth ,info.zenith_angle, \n",
    "                                  info.science_program, info.timespan.begin.jd,info.timespan.begin.mjd ]\n",
    "    except:\n",
    "        print(\">>>   Unexpected error:\", sys.exc_info()[0])\n",
    "        info_timespan_begin_to_string = \"2021-01-01 00:00:00.00\"\n",
    "        info_timespan_end_to_string = \"2051-01-01 00:00:00.00\"\n",
    "        info_timespan_begin_jd = 0\n",
    "        info_timespan_begin_mjd = 0\n",
    "        df_exp.loc[count] = [info.id, info.obs_id, info.day_obs, info.seq_num, \n",
    "                                  pd.to_datetime(info_timespan_begin_to_string),\n",
    "                                  pd.to_datetime(info_timespan_end_to_string), \n",
    "                                  info.observation_type, info.observation_reason, info.target_name, \n",
    "                                  info.physical_filter, info.zenith_angle, \n",
    "                                  info.exposure_time,info.tracking_ra, info.tracking_dec, \n",
    "                                  info.sky_angle,info.azimuth ,info.zenith_angle, \n",
    "                                  info.science_program, info_timespan_begin_jd, info_timespan_begin_mjd ]\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a6240-a01b-4876-bc5a-30441b03b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-cast the `id`, `day_obs`, and `seq_num` rows as `int`'s:\n",
    "df_exp = df_exp.astype({\"id\": int,'day_obs': int,'seq_num':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71ed48-b6ff-43a9-aa91-98e5cd835e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace `NaN`'s in the `ra` and `dec` columns with zero.  \n",
    "# (`NaN`'s in `ra`, `dec` wreak havoc for the healpix tools defined in Section 1.2 above.) \n",
    "# ***(Maybe no longer necessary?)***\n",
    "\n",
    "df_exp['ra'] = df_exp['ra'].fillna(0)\n",
    "df_exp['dec'] = df_exp['dec'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121311c-592e-497a-8eca-8611374087fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add healpix info \n",
    "# ***(Maybe no longer necessary?)***:\n",
    "df_exp.loc[:,'HPX64'] = getipix(64, df_exp.loc[:,'ra'].values, df_exp.loc[:,'dec'].values)\n",
    "df_exp.loc[:,'HPX16'] = getipix(16, df_exp.loc[:,'ra'].values, df_exp.loc[:,'dec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1097-23b6-482a-9560-d55e38d3fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To add the field_name to df_exp, perform a left join of df_field on df_exp\n",
    "# *** (Note use of HPX16, but maybe that part is no longer necessary?)***:\n",
    "\n",
    "# First, perform the merge...\n",
    "# (Before we had the table of fields, we matched by HPX16 value.)\n",
    "#df_merged = df_exp.merge(df_field, on='HPX16', how='left', suffixes=('', '_fld'))\n",
    "df_merged = skyMatchCatManytoOne(df_exp, 'ra', 'dec', df_field, 'RA', 'DEC', 3.)\n",
    "\n",
    "# Next, define a function to fill NaN values in 'Field' with 'HPX16' for unmatched exposures\n",
    "def fill_field(row):\n",
    "    if pd.isna(row['Field']):\n",
    "        return 'HPX16_'+str(row['HPX16'])  # Convert to string if you want 'Field' to be string type\n",
    "    return row['Field']\n",
    "\n",
    "# Apply the function to fill NaN values\n",
    "df_merged['Field'] = df_merged.apply(fill_field, axis=1)\n",
    "\n",
    "# Remove extraneous columns from df_merged\n",
    "#df_merged = df_merged.drop(['RA', 'DEC', 'stellarDens', 'E_BV', 'HPX64_fld', 'HPX16_fld'], axis=1)\n",
    "df_merged = df_merged.drop(['RA', 'DEC', 'stellarDens', 'E_BV', 'HPX64_b', 'HPX16_b'], axis=1)\n",
    "\n",
    "# Rename 'Field' column to 'field_name'\n",
    "df_merged = df_merged.rename(columns={'Field': 'field_name'})\n",
    "\n",
    "# Rename df_merged to df_exp\n",
    "df_exp = df_merged\n",
    "\n",
    "## Display updated df_exp\n",
    "#df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbedc7-be1a-42c3-aaf6-255b1f4157aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936cc2b-e14d-44f5-88a3-f126e60d13c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf17787-78d4-49b7-a766-cb86c37b484e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c0407-4162-4daf-803f-acc7494877e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81288beb-3c14-468e-a120-83c69ec22af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#butler_sky = dafButler.Butler(repo, collections=collection_sky)\n",
    "#registry_sky = butler_sky.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273d907-634d-4e18-84bb-c4771edccac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dt in sorted(registry_sky.queryDatasetTypes('*sky*')):\n",
    "#     print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1368c3b-bf35-43eb-afa1-009dc4f5e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_skymap = registry_sky.getDatasetType('skyMap')\n",
    "#print(dt_skymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463810d-0eaf-44ce-918c-ed894bf3e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_skymap.storageClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cdaba-79df-45b3-94dc-fd653df5430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skymap = butler_sky.get('skyMap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0f54d-3596-42fd-bedd-d76597380afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#butler_sky.collections.query_info('LATISS/prompt/output-2024-08-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fd9ba-b72b-4014-b103-6b051441b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try weekly_2024_37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e88fb-8601-4ab3-9cb8-8b29c7a169f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34c3f2-d5a5-4b34-82ee-1f695fc7727f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526907-43e8-47e8-a307-1d8323c3e886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d1187-c7cb-4b22-ae6a-b76ec52a2dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60089cb-f4b1-41f5-bf7c-96540f4305f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52043b-a9ee-402d-8ad8-20887e1e2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the tract id as a column in df_exp.  \n",
    "# First, access the skymap, then add the tract to the df_exp DataFrame\n",
    "# (this can take SEVERAL minutes!):\n",
    "\n",
    "butler_sky = dafButler.Butler(repo, collections=collection_sky)\n",
    "registry_sky = butler_sky.registry\n",
    "skymap = butler_sky.get('skyMap', skymap=skymap_name)\n",
    "\n",
    "df_exp['tract'] = df_exp.apply(lambda row: find_tract(row['ra'], row['dec'], skymap), axis=1)\n",
    "#df_exp['tract']  = df_exp.apply(lambda row: return_string_of_tracts_overlapping_exposure_plus(row['id'], instrument, repo, collection_sky, skymap_name, row['ra'], row['dec']), axis=1)\n",
    "#df_exp['tract']  = df_exp.apply(lambda row: return_string_of_tracts_overlapping_exposure_plus(row['id'], instrument, butler_sky, skymap, row['ra'], row['dec']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeeed89-d52d-4da4-bff6-a07fa875bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a `DataFrame` containing just the science exposures:\n",
    "df_sci = df_exp[df_exp.type == 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf3184-becb-40f8-820a-1cfca59f07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Analyse results of the exposure query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fa41a-7ac4-4236-9d77-d747052a37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For the rest of the notebook, we do not want to restrict the the number of rows displayed for\n",
    "# pandas tabular information.  Therefore, let us turn off the pandas maximum rows setting.  \n",
    "# We will turn it back on at the end of the notebook.\n",
    "\n",
    "# Change the display.max_rows option\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a822256-c7e2-4213-baf0-501cf8302779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.2 Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16b39a-cd20-471c-8e2d-fe7dd4fa6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.2.1 All Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93a02d-e5b4-4cc3-b7aa-2aa423c29b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total counts\n",
    "df_exp_grouped_type = df_exp.groupby(\"type\").count()[\"id\"].reset_index().rename(columns={'id': 'number'})\n",
    "df_exp_grouped_type.rename(columns={'number': 'n_exp'}, inplace=True)\n",
    "#df_exp_grouped_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fd38f-14af-4799-8916-b91a2af3b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group df_exp by 'type', col_filter.\n",
    "df_exp_grouped = df2Dgroupby(df_exp, 'type', col_filter, col_id, True)\n",
    "#df_exp_grouped\n",
    "\n",
    "# Pivot result, reset index, and rename the axis.\n",
    "df_exp_pivoted = df_exp_grouped.pivot(index=['type'], columns='filter', values='number')\n",
    "df_exp_pivoted = df_exp_pivoted.reset_index()\n",
    "df_exp_pivoted.rename_axis(columns=None, inplace=True)\n",
    "#df_exp_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb209d08-c012-44c7-a6a9-537cce2f4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform LEFT JOIN for df_exp_grouped_type, df_exp_pivoted on 'type'.\n",
    "df_exp_merged = pd.merge(df_exp_grouped_type, df_exp_pivoted, on=['type'], how='left')\n",
    "#df_exp_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a79f4e-8741-422d-930a-b7edaf1b8b5b",
   "metadata": {},
   "source": [
    "## Exposure Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11f6f4-deed-41de-ba13-12fd11936fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a \"TOTAL\" row at the end of df_exp_merged to contain the summed totals\n",
    "#  of each numerical column.\n",
    "\n",
    "# Instantiate df_totals dataframe.\n",
    "df_totals = pd.DataFrame({'type': ['TOTAL']})\n",
    "\n",
    "# Get the sum of each numerical column and add it as a new row.\n",
    "for col in df_exp_merged.select_dtypes(include='number'):\n",
    "    df_totals[col] = [df_exp_merged[col].sum()]\n",
    "\n",
    "df_final = pd.concat([df_exp_merged, df_totals], ignore_index=True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20cfc7-f3dc-479f-88d2-cd0871afd225",
   "metadata": {},
   "source": [
    "## Science Exposure Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f98b3-b166-4a06-8def-71eee1216130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.2.2 All Science Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd31b2-8e59-4902-b1a1-e190555661cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, group by col_sciprog, col_target, 'tract', and col_filter.\n",
    "df_sci_grouped = df4Dgroupby(df_sci, col_sciprog, col_target, 'tract', col_filter, col_id, True)\n",
    "#df_sci_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac007d6-5b1e-4fb6-b5d3-494720c72b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next, pivot the dataframe, reset the index, and rename the axis.\n",
    "#  Kudos to Poe.com AI assistant.\n",
    "df_sci_pivoted = df_sci_grouped.pivot(index=[col_sciprog, col_target, 'tract'], columns=col_filter, values='number')\n",
    "df_sci_pivoted = df_sci_pivoted.reset_index()\n",
    "df_sci_pivoted.rename_axis(columns=None, inplace=True)\n",
    "#df_sci_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ddab3-3d6d-4189-8366-9be53a228ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next,group by col_sciprog, col_target, and 'tract'.\n",
    "df_sci_grouped_tract = df3Dgroupby(df_sci, col_sciprog, col_target, 'tract', col_id, False)\n",
    "df_sci_grouped_tract.rename(columns={'number': 'n_exp'}, inplace=True)\n",
    "#df_sci_grouped_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75449f39-3e82-487e-b7d5-a74924dd4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next, perform a LEFT JOIN for df_sci_grouped_tract, df_sci_pivoted on col_sciprog, col_target.\n",
    "df_sci_merged = pd.merge(df_sci_grouped_tract, df_sci_pivoted, on=[col_sciprog, col_target, 'tract'], how='left')\n",
    "#df_sci_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15ef48-cca4-4727-9efc-08f3e4d500dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Due to dithering and/or pointing errors, some (col_sciprog, col_target) pairs\n",
    "#  may have multiple matches to the closest tract to the beam sight of the exposure.\n",
    "#  Here, we combine combine all the matches for each (col_sciprog, col_target) pair\n",
    "#  into a single dataframe row.\n",
    "# Kudos again to Poe.com AI assistant for this solution!\n",
    "\n",
    "#  Group df_sci_merged by (col_sciprog, col_target) pair.\n",
    "df_sci_merged_grp = df_sci_merged.groupby([col_sciprog, col_target])\n",
    "\n",
    "#  Identify all columns containing numerical values; these will be summed for \n",
    "#   each (col_sciprog, col_target) pair.\n",
    "numeric_cols = df_sci_merged.select_dtypes(include='number').columns.tolist()\n",
    "#  But remove the 'tract' column; we treat this numerical column differently.\n",
    "numeric_cols.remove('tract')\n",
    "\n",
    "#  And now combined tracts for each each (col_sciprog, col_target) pair, \n",
    "#   creating a comma-separated string for (col_sciprog, col_target) pairs\n",
    "#   that have multiple tract matches.\n",
    "df_sci_combined = df_sci_merged_grp.agg({\n",
    "    'tract': lambda x: ','.join(map(str, sorted(x, key=lambda y: df_sci_merged.loc[df_sci_merged['tract'] == y, 'n_exp'].values[0], reverse=True))),\n",
    "    **{col: 'sum' for col in numeric_cols}\n",
    "}).reset_index()\n",
    "\n",
    "#df_sci_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558e507-a828-4836-8e06-686217138a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a \"TOTAL\" row at the end of df_sci_merged to contain the summed totals\n",
    "#  of each numerical column.\n",
    "\n",
    "# Instantiate df_totals dataframe.\n",
    "df_totals = pd.DataFrame({col_sciprog: ['TOTAL'], col_target: [''], 'tract': ['']})\n",
    "\n",
    "# Get the sum of each numerical column and add it as a new row\n",
    "for col in df_sci_combined.select_dtypes(include='number'):\n",
    "    df_totals[col] = [df_sci_combined[col].sum()]\n",
    "\n",
    "df_final = pd.concat([df_sci_combined, df_totals], ignore_index=True)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976c6fc-704a-4985-8d11-40b3286c809a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T18:58:55.648531Z",
     "iopub.status.busy": "2024-08-25T18:58:55.648411Z",
     "iopub.status.idle": "2024-08-25T18:58:55.650278Z",
     "shell.execute_reply": "2024-08-25T18:58:55.649960Z",
     "shell.execute_reply.started": "2024-08-25T18:58:55.648520Z"
    }
   },
   "source": [
    "***The tracts listed are those that overlap the beam sight of the exposures associated with a given (`science_program`, `target`) pair.  Due to the extent of the focal plane, there may be additional tracts associated with a given exposure pointing; these are not recorded here.*** \n",
    "\n",
    "***That said, dithering and/or pointing errors may mean that different exposures associated with a (`science_program`, `target`) pair may be matched with different tracts.  In that case, a comma-separated list of tracts is provided in the above table for that (`science_program`, `target`) pair; the tract list is arranged in order of decreasing number of exposures associated with that tract.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3596b4c-4ef6-4592-9d68-772873f910ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.3 Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544028ca-64ef-492e-ae6f-b460d65a652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.3.1 Target RA, DEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48fd5a-9a84-405f-9308-a8faede7c037",
   "metadata": {},
   "source": [
    "## Science Target Exposure Sky Positions (RA, DEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a255c90-e086-4bf4-8796-fdfd7b08f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sci.sort_values(by='field_name', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot in Cartesian coordinates\n",
    "if df_sci[col_target].dtype == \"int64\" or df_sci[col_target].dtype == \"float64\":\n",
    "    # We create and annotate the plot one way if col_target is numerical...\n",
    "    ax = sns.scatterplot(data=df_sci, x=\"ra\", y=\"dec\", hue=col_target, palette='viridis', s=100)\n",
    "else:\n",
    "    # Otherwise, if col_target is a string or similar, we create and annotate the plot this way...\n",
    "    #  (This could be done better.  As it is, the spacing is very crowded.)\n",
    "    ax = sns.scatterplot(data=df_sci, x=\"ra\", y=\"dec\", s=100)\n",
    "    prev_label = 'XXXXXX'\n",
    "    for i, label in enumerate(df_sci[col_target]):\n",
    "        if label != prev_label:\n",
    "            ax.annotate(label, (df_sci['ra'].iloc[i], df_sci['dec'].iloc[i]), fontsize=6)\n",
    "            prev_label = label\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('DEC')\n",
    "title = \"\"\"RA, DEC positions of %s Observations of Different Science Program Targets on %s\"\"\" %(instrument, day_obs)\n",
    "plt.title(title)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73916fe-4185-42a1-85cb-7c81589b248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.3.2 Target vs. zenith_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09c218-5a70-4f85-941e-fffadb986945",
   "metadata": {},
   "source": [
    "## Science Target Exposure Zenith Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cf1ba-4e0d-429f-8d9b-52fa29f08658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.scatterplot(data=df_sci, x=\"zenith_angle\", y=col_target)\n",
    "\n",
    "# Add labels and title\n",
    "#plt.xlabel('Zenith Angle')\n",
    "#plt.ylabel('Target')\n",
    "title = \"\"\"Zenith distances for of %s Observations of Different Science Program Targets on %s\"\"\" %(instrument, day_obs)\n",
    "plt.title(title)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb978e-17c0-4ad7-bb16-41fa4e239567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.3.3 Target vs. MJD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79b958-bdc2-4872-b577-2198ac74c61e",
   "metadata": {},
   "source": [
    "## Science Target Exposure MJDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4139ce-9cd4-47d9-8288-681db988dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#ax = sns.scatterplot(data=df_sci, x=\"mjd\", y=\"target\")\n",
    "ax = sns.scatterplot(data=df_sci, x=\"mjd\", y=col_target)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('MJD')\n",
    "plt.ylabel(col_target)\n",
    "title = \"\"\"MJDs of %s Observations of Different Science Program Targets on %s\"\"\" %(instrument, day_obs)\n",
    "plt.title(title)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c4fb-a9f6-46fc-b986-3bd346bdd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 3.3.4 Target zenith angle vs. MJD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854a142-ed35-4a18-bcf1-39e72d3a7024",
   "metadata": {},
   "source": [
    "## Science Target Exposure Zenith Angles vs. MJD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d7e9a-a8c5-43e3-9fb3-f8b7cf5b6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sci.sort_values(by='mjd', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot in Cartesian coordinates\n",
    "if df_sci[col_target].dtype == \"int64\" or df_sci[col_target].dtype == \"float64\":\n",
    "    # We create and annotate the plot one way if col_target is numerical...\n",
    "    ax = sns.scatterplot(data=df_sci, x=\"mjd\", y=\"zenith_angle\", hue=col_target, palette='viridis', s=100)\n",
    "else:\n",
    "    # Otherwise, if col_target is a string or similar, we create and annotate the plot this way...\n",
    "    #  (This could be done better.  As it is, the spacing is very crowded.)\n",
    "    ax = sns.scatterplot(data=df_sci, x=\"mjd\", y=\"zenith_angle\", s=100)\n",
    "    prev_label = 'XXXXXX'\n",
    "    for i, label in enumerate(df_sci[col_target]):\n",
    "        if label != prev_label:\n",
    "            #mjd = df_sci['mjd'].iloc[i]\n",
    "            #za = df_sci['zenith_angle'].iloc[i]\n",
    "            #print(i, label, mjd, za)\n",
    "            ax.annotate(label, (df_sci['mjd'].iloc[i], df_sci['zenith_angle'].iloc[i]), fontsize=6)\n",
    "            prev_label = label\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('MJD')\n",
    "plt.ylabel('Zenith Angle')\n",
    "title = \"\"\"Zenith Angle vs. MJD of %s Observations of Different Science Program Targets on %s\"\"\" %(instrument, day_obs)\n",
    "plt.title(title)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9420ed8-4e09-41a2-b333-3f958b5e2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Final cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4ccc9-25a5-4ddc-853c-01da52b614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the display.max_rows option to the original default\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c94801-1bfc-4f2b-94e2-05842a92766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop automatic execution of cells before reaching the Sandbox...\n",
    "#sys.exit(\"Notebook execution stopped here.\")\n",
    "\n",
    "raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267298f2-a8ca-4c0b-8f13-67aae20998cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f95453-9665-42ee-b852-e8df40e4e605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d8bec-fb4c-4200-aaae-a0d97f5f18c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
