{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b4a6e5-561a-4ca3-adc8-55880e007447",
   "metadata": {},
   "source": [
    "**Description:** A prototype dashboard for identifying table columns from with issues.  This version focuses on ComCom data, but should be useful for LSSTcam commissioning data processing as well.\n",
    "\n",
    "See Jira issue <a href=https://rubinobs.atlassian.net/browse/SP-861>SP-861</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908fae9f-45c9-4558-9492-2de277964fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:29:05.986315Z",
     "iopub.status.busy": "2025-09-24T23:29:05.986186Z",
     "iopub.status.idle": "2025-09-24T23:29:05.989263Z",
     "shell.execute_reply": "2025-09-24T23:29:05.988871Z",
     "shell.execute_reply.started": "2025-09-24T23:29:05.986300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## 1.1 User-defined values\n",
    "\n",
    "instrument = 'LSSTCam'\n",
    "collectionName = 'LSSTCam/runs/DRP/20250501_20250609/w_2025_26/DM-51580'\n",
    "table_type = 'objectTable' # currently, just 'objectTable' is recognized.  'sourceTable' is in the process of being implemented.\n",
    "#table_type = 'sourceTable' # currently, just 'objectTable' is recognized.  'sourceTable' (or possibly 'recalibrated_star') is in the process of being implemented.\n",
    "\n",
    "# Use downloaded CSV files (\"True\") or query the EFD (\"False\"):\n",
    "useCSV = True\n",
    "\n",
    "if useCSV:\n",
    "    if instrument == 'LSSTComCam':\n",
    "        tableSummaryDir = '../../../SP-861/pipe_tasks/LSSTComCam'\n",
    "    elif instrument == 'LSSTCam':\n",
    "        tableSummaryDir = '../../../SP-861/pipe_tasks/LSSTCam'\n",
    "else:\n",
    "    efd_name = 'usdfdev_efd'\n",
    "    db_name = 'lsst.dm'\n",
    "    table_name = \"\"\"lsst.dm.%s\"\"\" % (table_type)\n",
    "    time_lo = '2025-04-08'\n",
    "    time_hi = '2025-12-31'\n",
    "    \n",
    "# Verbosity level:  0, 1, 2, 3...  higher values yield more output to screen\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65572d28-52a1-463c-b1b0-d645b565fbcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:29:05.989773Z",
     "iopub.status.busy": "2025-09-24T23:29:05.989645Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## 1.2 Import packages\n",
    "\n",
    "# Generic python packages\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re # Import the regular expression module\n",
    "from IPython.display import display\n",
    "\n",
    "# Astropy-related packages\n",
    "from astropy.time import Time\n",
    "\n",
    "# LSST packages\n",
    "from lsst_efd_client import EfdClient\n",
    "import lsst.daf.butler as dafButler\n",
    "from lsst.resources import ResourcePath\n",
    "\n",
    "# Converts matplotlib.category INFO messages to WARNINGS\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Set filter warnings to \"ignore\" to avoid a lot of \"logorrhea\" to the screen:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set pandas to show all rows...\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d2c62-0bcf-4ad4-bda2-e79f9f513e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.3. Define methods and classes...\n",
    "\n",
    "# Define a class to stop \"Run All\" at a code cell containing the command \"raise StopExecution\":\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def create_tract_markup_color_dict(repo, collection, skymap_name, top_n=20, plot=False):\n",
    "    \"\"\"\n",
    "    Query LSSTCam science exposures, determine tract IDs, and return:\n",
    "      - tract_dict_top: dict mapping tractId -> most frequent target_name (restricted to top targets)\n",
    "      - marker_dict_top: dict mapping target_name -> matplotlib marker symbol\n",
    "      - color_dict_top: dict mapping target_name -> matplotlib color\n",
    "\n",
    "    Optionally plots the sky distribution of the top targets using Astropy for coordinate handling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    repo : str\n",
    "        Butler repository path (e.g., '/repo/main')\n",
    "    collection : str\n",
    "        Butler collection name (e.g., 'LSSTCam/defaults')\n",
    "    skymap_name : str\n",
    "        Name of the skymap to load (e.g., 'lsst_cells_v1')\n",
    "    top_n : int, optional\n",
    "        Number of top target_names to consider (default=20)\n",
    "    plot : bool, optional\n",
    "        If True, produce a matplotlib Aitoff sky plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tract_dict_top : dict\n",
    "    marker_dict_top : dict\n",
    "    color_dict_top : dict\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 0: Important necessary packages ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from astropy.coordinates import SkyCoord\n",
    "    import astropy.units as u\n",
    "    from lsst.rsp import get_tap_service\n",
    "    import lsst.daf.butler as dafButler\n",
    "    from lsst.geom import SpherePoint, degrees\n",
    "\n",
    "    # --- Step 1: Query ConsDB ---\n",
    "    cdb = get_tap_service(\"consdbtap\")\n",
    "    assert cdb is not None, \"Could not connect to consdbtap service.\"\n",
    "\n",
    "    query = \"\"\"SELECT img_type, science_program, target_name, scheduler_note, \n",
    "                      s_ra, s_dec, band, exp_midpt, exp_midpt_mjd, exp_time, can_see_sky \n",
    "               FROM cdb_lsstcam.visit1 \n",
    "               WHERE img_type='science' AND can_see_sky='True'\"\"\"\n",
    "    results = cdb.search(query).to_table()\n",
    "    df_sci_exp = results.to_pandas()\n",
    "\n",
    "    # Normalize empty target_name values\n",
    "    df_sci_exp['target_name'] = (\n",
    "        df_sci_exp['target_name']\n",
    "        .replace('', np.nan)\n",
    "        .fillna('<blank>')\n",
    "    )\n",
    "\n",
    "    # --- Step 2: Load skymap ---\n",
    "    butler = dafButler.Butler(repo, collections=collection)\n",
    "    skyMap = butler.get('skyMap', skymap=skymap_name)\n",
    "\n",
    "    # --- Step 3: Add tractId ---\n",
    "    def get_tract_id(row):\n",
    "        try:\n",
    "            coord = SpherePoint(row['s_ra'], row['s_dec'], degrees)\n",
    "            tractInfo = skyMap.findTract(coord)\n",
    "            return tractInfo.getId()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df_sci_exp['tractId'] = df_sci_exp.apply(get_tract_id, axis=1)\n",
    "\n",
    "    # --- Step 4: Top N targets ---\n",
    "    top_targets = (\n",
    "        df_sci_exp['target_name']\n",
    "        .value_counts()\n",
    "        .head(top_n)\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    df_sci_exp_top = df_sci_exp[df_sci_exp['target_name'].isin(top_targets)]\n",
    "\n",
    "    # --- Step 5: tract_dict_top ---\n",
    "    counts = df_sci_exp_top.groupby(['tractId', 'target_name']).size().reset_index(name='count')\n",
    "    tract_dict_top = (\n",
    "        counts.sort_values(['tractId', 'count'], ascending=[True, False])\n",
    "              .drop_duplicates(subset='tractId')\n",
    "              .set_index('tractId')['target_name']\n",
    "              .to_dict()\n",
    "    )\n",
    "\n",
    "    # --- Step 6: Count unique tracts per target_name ---\n",
    "    tract_counts = (\n",
    "        df_sci_exp.groupby('target_name')['tractId']\n",
    "        .nunique()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # --- Step 7: Sort top targets by tract count ---\n",
    "    legend_order = sorted(top_targets, key=lambda t: tract_counts.get(t, 0), reverse=True)\n",
    "\n",
    "    # --- Step 8: Assign markers/colors in sorted order ---\n",
    "    marker_styles = ['o', 's', '^', 'v', '<', '>', 'D', 'p', '*', 'h', 'H', 'X', 'd', '|', '_', '+', 'x', 'P', '1', '2']\n",
    "    color_cycle = plt.cm.tab20.colors\n",
    "\n",
    "    marker_dict_top = {t: marker_styles[i % len(marker_styles)] for i, t in enumerate(legend_order)}\n",
    "    color_dict_top = {t: color_cycle[i % len(color_cycle)] for i, t in enumerate(legend_order)}\n",
    "\n",
    "    print\n",
    "\n",
    "    # Add \"Other\" category\n",
    "    marker_dict_top[\"Other\"] = '.'\n",
    "    color_dict_top[\"Other\"] = 'lightgray'\n",
    "\n",
    "    # Does not seem to be working:\n",
    "    ## Add \"All\" category\n",
    "    #marker_dict_top[\"All\"] = '.'\n",
    "    #color_dict_top[\"All\"] = 'darkgray'\n",
    "\n",
    "    # --- Step 9: Optional plotting ---\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        ax = fig.add_subplot(111, projection=\"aitoff\")\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Plot top targets in sorted order\n",
    "        for target in legend_order:\n",
    "            subset = df_sci_exp[df_sci_exp['target_name'] == target]\n",
    "            coords = SkyCoord(ra=subset['s_ra'].values * u.deg,\n",
    "                              dec=subset['s_dec'].values * u.deg,\n",
    "                              frame='icrs')\n",
    "            # Wrap RA and flip for Rubin convention\n",
    "            ra_rad = -coords.ra.wrap_at(180 * u.deg).radian\n",
    "            dec_rad = coords.dec.radian\n",
    "\n",
    "            ax.scatter(\n",
    "                ra_rad,\n",
    "                dec_rad,\n",
    "                marker=marker_dict_top[target],\n",
    "                color=color_dict_top[target],\n",
    "                label=f\"{target} ({tract_counts.get(target, 0)} tracts)\",\n",
    "                alpha=0.7,\n",
    "                s=20\n",
    "            )\n",
    "\n",
    "        # Plot \"Other\" targets last\n",
    "        subset_other = df_sci_exp[~df_sci_exp['target_name'].isin(top_targets)]\n",
    "        if not subset_other.empty:\n",
    "            coords = SkyCoord(ra=subset_other['s_ra'].values * u.deg,\n",
    "                              dec=subset_other['s_dec'].values * u.deg,\n",
    "                              frame='icrs')\n",
    "            ra_rad = -coords.ra.wrap_at(180 * u.deg).radian\n",
    "            dec_rad = coords.dec.radian\n",
    "\n",
    "            other_tracts = subset_other['tractId'].nunique()\n",
    "\n",
    "            ax.scatter(\n",
    "                ra_rad,\n",
    "                dec_rad,\n",
    "                marker=marker_dict_top[\"Other\"],\n",
    "                color=color_dict_top[\"Other\"],\n",
    "                label=f\"Other ({other_tracts} tracts)\",\n",
    "                alpha=0.5,\n",
    "                s=10\n",
    "            )\n",
    "\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4, fontsize='small')\n",
    "        ax.set_title(f\"Top {top_n} LSSTCam Science Fields in ConsDB by Tract\", pad=20)\n",
    "        plt.show()\n",
    "\n",
    "    return tract_dict_top, marker_dict_top, color_dict_top\n",
    "\n",
    "\n",
    "def instrument_create_tract_markup_color_dict(instrument):\n",
    "\n",
    "    if instrument == 'LSSTComCam':\n",
    "        \n",
    "        # From Slide 9 of https://docs.google.com/presentation/d/1NGzrT4t6wDGQ2-2a8rjioToquhx2vOP_KJTrPiCrDDY/edit#slide=id.g33de3f5c849_6_250\n",
    "        tract_dict={453: '47 Tuc', \n",
    "                    454: '47 Tuc',\n",
    "                   4849: 'ECDFS', \n",
    "                   5063: 'ECDFS',\n",
    "                   4848: 'ECDFS', \n",
    "                   2394: 'EDFS', \n",
    "                   2234: 'EDFS',\n",
    "                   4016: 'Fornax', \n",
    "                   4017: 'Fornax', \n",
    "                   4218: 'Fornax', \n",
    "                   4217: 'Fornax', \n",
    "                   5525: 'Rubin_SV_095-25', \n",
    "                   5526: 'Rubin_SV_095-25', \n",
    "                   7611: 'Seagull', \n",
    "                   7610: 'Seagull', \n",
    "                   7850: 'Seagull',\n",
    "                   10463: 'Rubin_SV_38_7', \n",
    "                   10464: 'Rubin_SV_38_7', \n",
    "                   10704: 'Rubin_SV_38_7'\n",
    "                   }\n",
    "\n",
    "        marker_dict={'47 Tuc': 'o', \n",
    "                     'ECDFS': 'v', \n",
    "                     'EDFS': '^', \n",
    "                     'Fornax': 's', \n",
    "                     'Rubin_SV_095-25': '*', \n",
    "                     'Seagull': 'd', \n",
    "                     'Rubin_SV_38_7': 'P', \n",
    "                     'Other': 'X'\n",
    "                    }\n",
    "\n",
    "        color_dict= {'47 Tuc': 'b', \n",
    "                     'ECDFS': 'g', \n",
    "                     'EDFS': 'r', \n",
    "                     'Fornax': 'c', \n",
    "                     'Rubin_SV_095-25': 'm', \n",
    "                     'Seagull': 'y', \n",
    "                     'Rubin_SV_38_7': 'k', \n",
    "                     'Other': 'gray'\n",
    "                    }\n",
    "\n",
    "    elif instrument == 'LSSTCam':\n",
    "    \n",
    "        try:\n",
    "\n",
    "            # Try to grab from the ConsDB\n",
    "\n",
    "            tract_dict, marker_dict, color_dict = create_tract_markup_color_dict(\n",
    "                repo='/repo/main',\n",
    "                collection='LSSTCam/defaults',\n",
    "                skymap_name='lsst_cells_v1',\n",
    "                top_n=20,\n",
    "                plot=False   # Set to True if you want an Aitoff pojection plot of the top_n targets\n",
    "            )\n",
    "\n",
    "        except:\n",
    "        \n",
    "            # If not, here is an outdated list the Times Square TargetViewer\n",
    "   \n",
    "            tract_dict={9813 : 'COSMOS', \n",
    "                        1291 : 'Carina',\n",
    "                        1292 : 'Carina',\n",
    "                        1293 : 'Carina',\n",
    "                        1417 : 'Carina',\n",
    "                        9591 : 'DESI_SV3_R1',\n",
    "                        9348 : 'DESI_SV3_R1',\n",
    "                        10804 : 'M49',\n",
    "                        10321 : 'M49',\n",
    "                        10563 : 'M49',\n",
    "                        10562 : 'M49',\n",
    "                        10320 : 'M49',\n",
    "                        10803 : 'M49',\n",
    "                        6325: 'New_Horizons', \n",
    "                        6324: 'New_Horizons', \n",
    "                        6096: 'New_Horizons', \n",
    "                        6323: 'New_Horizons', \n",
    "                        6555: 'New_Horizons', \n",
    "                        6554: 'New_Horizons', \n",
    "                        3361: 'Prawn',\n",
    "                        3360: 'Prawn',\n",
    "                        3359: 'Prawn',\n",
    "                        3362: 'Prawn',\n",
    "                        3176: 'Prawn',\n",
    "                        3175: 'Prawn',\n",
    "                        8401:  'Rubin_SV_212_-7',\n",
    "                        8400:  'Rubin_SV_212_-7',\n",
    "                        8399:  'Rubin_SV_212_-7',\n",
    "                        8641:  'Rubin_SV_212_-7',\n",
    "                        8158:  'Rubin_SV_212_-7',\n",
    "                        8883:  'Rubin_SV_212_-7',\n",
    "                        6740:  'Rubin_SV_216_-17',\n",
    "                        6973:  'Rubin_SV_216_-17',\n",
    "                        3533:  'Rubin_SV_225_-40',\n",
    "                        3534:  'Rubin_SV_225_-40',\n",
    "                        3725:  'Rubin_SV_225_-40',\n",
    "                        3346:  'Rubin_SV_225_-40',\n",
    "                        3724:  'Rubin_SV_225_-40',\n",
    "                        3345:  'Rubin_SV_225_-40',\n",
    "                        3532:  'Rubin_SV_225_-40',\n",
    "                        5857:  'Trifid-Lagoon',\n",
    "                        5636:  'Trifid-Lagoon',\n",
    "                        5635:  'Trifid-Lagoon',\n",
    "                        5634:  'Trifid-Lagoon',\n",
    "                        5858:  'Trifid-Lagoon',\n",
    "                        1189:  'Alpha_Centauri',\n",
    "                        1312:  'Alpha_Centauri',\n",
    "                        1438:  'Alpha_Centauri',\n",
    "                        1439:  'Alpha_Centauri_at_the_edge'\n",
    "                       }\n",
    "\n",
    "            marker_dict={'COSMOS': 'o', \n",
    "                         'Carina': 'v', \n",
    "                         'DESI_SV3_R1': '^', \n",
    "                         'M49': 's', \n",
    "                         'New_Horizons': '*', \n",
    "                         'Prawn': 'd', \n",
    "                         'Rubin_SV_212_-7': 'P',\n",
    "                         'Rubin_SV_216_-17': '<', \n",
    "                         'Rubin_SV_225_-40': '>', \n",
    "                         'Trifid-Lagoon': '8', \n",
    "                         'Alpha_Centauri': 'h', \n",
    "                         'Alpha_Centauri_at_the_edge': 'H', \n",
    "                         'Other': 'X'\n",
    "                        }\n",
    "\n",
    "            color_dict= {'COSMOS': 'b', \n",
    "                         'Carina': 'g', \n",
    "                         'DESI_SV3_R1': 'r', \n",
    "                         'M49': 'c', \n",
    "                         'New_Horizons': 'm', \n",
    "                         'Prawn': 'pink', \n",
    "                         'Rubin_SV_212_-7': 'k', \n",
    "                         'Rubin_SV_216_-17': 'c', \n",
    "                         'Rubin_SV_225_-40': 'aquamarine', \n",
    "                         'Trifid-Lagoon': 'powderblue', \n",
    "                         'Alpha_Centauri': 'violet', \n",
    "                         'Alpha_Centauri_at_the_edge': 'darkviolet',              \n",
    "                         'Other': 'gray'\n",
    "                        }\n",
    "\n",
    "        return tract_dict, marker_dict, color_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddb124-d202-4583-a2f7-70dd55dffd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.3. (cont'd)  Define (yet more) methods and classes... \n",
    "\n",
    "# Developed using ChatGPT-5 on Microsoft Copilot:\n",
    "def plot_hist_and_scatter(\n",
    "    df, numeric_cols, filtername, table_type, collectionName,\n",
    "    marker_dict, color_dict, super_title,\n",
    "    output_file=None, show_inline=False,\n",
    "    base_row_height=None, max_height=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create histogram + scatter plots for given numeric columns, with auto figure height.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    numeric_cols : list\n",
    "        List of numeric column names to plot.\n",
    "    filtername : str\n",
    "        Filter name (e.g., 'g', 'none', 'all') for labeling.\n",
    "    table_type : str\n",
    "        'objectTable' or 'sourceTable'.\n",
    "    collectionName : str\n",
    "        Name of the collection for titles.\n",
    "    marker_dict, color_dict : dict\n",
    "        Dictionaries mapping fieldnames to markers/colors.\n",
    "    super_title : str\n",
    "        Figure suptitle.\n",
    "    output_file : str or None\n",
    "        If provided, save figure to this file.\n",
    "    show_inline : bool\n",
    "        If True, display the plot in the notebook.\n",
    "    base_row_height : int or float or None\n",
    "        Height (in inches) per numeric column row. If None, chosen automatically:\n",
    "        - Larger for saved plots\n",
    "        - Smaller for previews\n",
    "    max_height : int or float or None\n",
    "        Maximum figure height in inches. If None, chosen automatically:\n",
    "        - Larger for saved plots\n",
    "        - Smaller for previews\n",
    "    \"\"\"\n",
    "    n_cols = len(numeric_cols)\n",
    "    global_min, global_max = -0.1, 1.1\n",
    "\n",
    "    # Auto-choose sizing defaults if not provided\n",
    "    if output_file:  # full saved plot\n",
    "        if base_row_height is None:\n",
    "            base_row_height = 4     # compact but readable\n",
    "        if max_height is None:\n",
    "            max_height = 250        # allow very tall figures\n",
    "    else:  # preview\n",
    "        if base_row_height is None:\n",
    "            base_row_height = 4     # compact but readable\n",
    "        if max_height is None:\n",
    "            max_height = 40         # cap for notebook display\n",
    "\n",
    "    # Auto-adjust figure height\n",
    "    fig_height = min(base_row_height * n_cols, max_height)\n",
    "\n",
    "    fig, axes = plt.subplots(n_cols, 2, figsize=(12, fig_height), sharex=False)\n",
    "    plt.subplots_adjust(top=0.98, bottom=0.05, hspace=0.25, wspace=0.3)\n",
    "\n",
    "    # Ensure axes is always 2D array for consistent indexing\n",
    "    if n_cols == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        ax_hist = axes[i, 0]\n",
    "        data = df[col].dropna()\n",
    "        # TEMPORARUY KLUDGE!!!:  Remove tracts for which the NaN_Fraction >= 1.00 for this col\n",
    "        data = data[data < 1.00]\n",
    "        if data.empty:\n",
    "            continue\n",
    "\n",
    "        # Stats\n",
    "        data_desc = data.describe(percentiles=[0.01, 0.99])\n",
    "        mean = data_desc['mean']\n",
    "        std = data_desc['std']\n",
    "        percent_01 = data_desc['1%']\n",
    "        median = data_desc['50%']\n",
    "        percent_99 = data_desc['99%']\n",
    "\n",
    "        # Histogram\n",
    "        ax_hist.hist(data, bins=50, alpha=0.75, range=(global_min, global_max))\n",
    "        xlabel = col.replace(\"_nan_fraction\", \"\")\n",
    "        if table_type == 'objectTable':\n",
    "            xlabel = f\"NaN Fraction in tract:    {xlabel}\"\n",
    "            ax_hist.set_ylabel('N_tracts')\n",
    "        elif table_type == 'sourceTable':\n",
    "            xlabel = f\"NaN Fraction in visit:    {xlabel}\"\n",
    "            ax_hist.set_ylabel('N_visits')\n",
    "        ax_hist.set_xlabel(xlabel)\n",
    "        ax_hist.grid(True, alpha=0.3)\n",
    "        ax_hist.axvline(mean, color='r', linestyle='--', alpha=0.8, label=f'Mean: {mean:.2f}')\n",
    "        ax_hist.axvline(percent_01, color='b', linestyle='--', alpha=0.8, label=f'1%: {percent_01:.2f}')\n",
    "        ax_hist.axvline(median, color='orange', linestyle='--', alpha=0.8, label=f'50%: {median:.2f}')\n",
    "        ax_hist.axvline(percent_99, color='g', linestyle='--', alpha=0.8, label=f'99%: {percent_99:.2f}')\n",
    "        stats_text = f'N: {len(data)}\\nσ: {std:.2f}'\n",
    "        ax_hist.text(0.10, 0.95, stats_text,\n",
    "                     transform=ax_hist.transAxes,\n",
    "                     verticalalignment='top',\n",
    "                     horizontalalignment='right',\n",
    "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        ax_hist.legend()\n",
    "\n",
    "        # Scatter\n",
    "        ax_scatter = axes[i, 1]\n",
    "        for fieldname in df['fieldname'].unique():\n",
    "            subset = df[df['fieldname'] == fieldname]\n",
    "            marker = marker_dict[fieldname]\n",
    "            color = color_dict[fieldname]\n",
    "            if table_type == 'objectTable':\n",
    "                ax_scatter.scatter(subset[col], subset['tract'],\n",
    "                                   marker=marker, color=color, label=fieldname, alpha=0.7)\n",
    "                ax_scatter.set_ylabel('Tract')\n",
    "            elif table_type == 'sourceTable':\n",
    "                ax_scatter.scatter(subset[col], subset['visit'],\n",
    "                                   marker=marker, color=color, label=fieldname, alpha=0.7)\n",
    "                ax_scatter.set_ylabel('Visit')\n",
    "        ax_scatter.set_xlabel(xlabel)\n",
    "        ax_scatter.grid(True, alpha=0.3)\n",
    "        ax_scatter.legend(title='Fieldname', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.xlim(global_min, global_max)\n",
    "    plt.suptitle(super_title, y=1.001, size=14, weight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Generated plot with {n_cols} rows (numeric columns) → saved to: {os.path.abspath(output_file)}\")\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "        if not output_file:\n",
    "            print(f\"Generated preview plot with {n_cols} rows (numeric columns) → displayed inline\")\n",
    "\n",
    "\n",
    "# Developed using ChatGPT-5 on Microsoft Copilot:\n",
    "\n",
    "def plot_nan_fraction_summary(\n",
    "    df, table_type, collectionName,\n",
    "    output_file=None, show_inline=False,\n",
    "    title_prefix=\"\",\n",
    "    max_height_per_row=0.25, base_height=4,\n",
    "    reference_order=None,\n",
    "    full_plot_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot NaN fraction summary scatterplot for given DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns '1%', '50%', '99%' and 'name'.\n",
    "    table_type : str\n",
    "        'objectTable' or 'sourceTable'.\n",
    "    collectionName : str\n",
    "        Name of the collection for titles.\n",
    "    output_file : str or None\n",
    "        If provided, save figure to this file.\n",
    "    show_inline : bool\n",
    "        If True, display the plot in the notebook.\n",
    "    title_prefix : str\n",
    "        Optional prefix for the plot title.\n",
    "    max_height_per_row : float\n",
    "        Height in inches per row for sizing the figure.\n",
    "    base_height : float\n",
    "        Minimum figure height in inches.\n",
    "    reference_order : list or None\n",
    "        Optional list of index labels to enforce a specific row order.\n",
    "    full_plot_path : str or None\n",
    "        Optional addition of full path to the output file the super title.\n",
    "    \"\"\"\n",
    "    # If a reference order is provided, reindex to match it (dropping missing)\n",
    "    if reference_order is not None:\n",
    "        df = df.loc[df.index.intersection(reference_order)]\n",
    "        df = df.reindex(reference_order)\n",
    "\n",
    "    n_rows = len(df)\n",
    "    fig_height = max(base_height, n_rows * max_height_per_row)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, fig_height))\n",
    "    plt.grid(True)\n",
    "\n",
    "    ax = sns.scatterplot(data=df, x=\"1%\", y=\"name\", color='b', label=\"1%\")\n",
    "    sns.scatterplot(data=df, x=\"50%\", y=\"name\", ax=ax, color='orange', label=\"50%\")\n",
    "    sns.scatterplot(data=df, x=\"99%\", y=\"name\", ax=ax, color='g', label=\"99%\")\n",
    "\n",
    "    plt.legend(title=\"Percentiles\")\n",
    "\n",
    "    # Force correct x-axis label\n",
    "    plt.xlabel(\"NaN Fraction\")\n",
    "\n",
    "    # Build title\n",
    "    if table_type == 'objectTable':\n",
    "        title_text = f\"{title_prefix} NaN Fraction in Tract\\nCollection: {collectionName}\"\n",
    "    elif table_type == 'sourceTable':\n",
    "        title_text = f\"{title_prefix} NaN Fraction in Visit\\nCollection: {collectionName}\"\n",
    "    else:\n",
    "        title_text = f\"{title_prefix} NaN Fraction\\nCollection: {collectionName}\"\n",
    "\n",
    "    # Append file path only for preview\n",
    "    if output_file is None and full_plot_path:\n",
    "        title_text += f\"\\nFull plot in {os.path.abspath(full_plot_path)}\"\n",
    "\n",
    "    plt.title(title_text)\n",
    "\n",
    "    # Adjust margins\n",
    "    plt.margins(y=0.0075 if n_rows > 10 else 0.1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_file:\n",
    "        fig.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Generated plot with {n_rows} rows (numeric columns) → saved to: {os.path.abspath(output_file)}\")\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "        if not output_file:\n",
    "            print(f\"Generated preview plot with {n_rows} rows (numeric columns) → displayed inline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ee1c3-0d3a-4356-997c-1aee47720410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c1783-1f4b-47a2-8c0e-95dbcdf9fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2.1. Retrieve data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d9ff-118a-4a87-befe-da56330e8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2.1.1.  Identify CSV files to read\n",
    "\n",
    "if useCSV:\n",
    "    \n",
    "    if table_type == 'objectTable':\n",
    "        pattern = os.path.join(tableSummaryDir, 'objectSummary.*.csv')\n",
    "    elif table_type == 'sourceTable':\n",
    "        pattern = os.path.join(tableSummaryDir, 'sourceSummary.*.csv')\n",
    "    else:\n",
    "        print(\"\"\"%s is unrecognized...  Stopping here!\"\"\" % (table_type))\n",
    "        raise StopExecution\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    if verbose > 2:\n",
    "        for f in files:\n",
    "            print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac418d3-e137-4c62-92e5-b469518e26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2.1.2.  Create pandas DataFrame that contains the data from all the CSV files\n",
    "\n",
    "# Per Gemini+2.5-Flash:\n",
    "\n",
    "if useCSV:\n",
    "    \n",
    "    # Create an empty list to store individual DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    # Regular expression to find the tract or visit number\n",
    "    # It looks for one or more digits (\\d+) between \"*Summary.\" and \".csv\"\n",
    "    if table_type == 'objectTable':\n",
    "        tract_regex = re.compile(r'objectSummary\\.(\\d+)\\.csv')\n",
    "    elif table_type == 'sourceTable':\n",
    "        visit_regex = re.compile(r'sourceSummary\\.(\\d+)\\.csv')\n",
    "    else:\n",
    "        print(\"\"\"%s is unrecognized...  Stopping here!\"\"\" % (table_type))\n",
    "        raise StopExecution\n",
    "\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            # Read each CSV file into a DataFrame\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract the filename from the full path\n",
    "            filename = os.path.basename(file_path)\n",
    "\n",
    "            if table_type == 'objectTable':\n",
    "                # Use regex to find the tract number\n",
    "                match = tract_regex.match(filename)\n",
    "                if match:\n",
    "                    tract_number = int(match.group(1)) # Convert the extracted string to an integer\n",
    "                    # Add the tract number as a new column to the temporary DataFrame\n",
    "                    df_temp['tract'] = tract_number\n",
    "                else:\n",
    "                    print(f\"Warning: Could not extract tract number from {filename}. 'tract' column for this file will be NaN.\")\n",
    "                    df_temp['tract'] = pd.NA # Assign a missing value if no tract is found\n",
    "            elif table_type == 'sourceTable':\n",
    "                # Use regex to find the tract number\n",
    "                match = visit_regex.match(filename)\n",
    "                if match:\n",
    "                    visit_number = int(match.group(1)) # Convert the extracted string to an integer\n",
    "                    # Add the tract number as a new column to the temporary DataFrame\n",
    "                    df_temp['visit'] = visit_number\n",
    "                else:\n",
    "                    print(f\"Warning: Could not extract visit number from {filename}. 'visit' column for this file will be NaN.\")\n",
    "                    df_temp['visit'] = pd.NA # Assign a missing value if no tract is found\n",
    "\n",
    "            all_dfs.append(df_temp)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: {file_path} is empty and will be skipped.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    if all_dfs:\n",
    "        df = pd.concat(all_dfs, ignore_index=True)\n",
    "        if verbose > 1:\n",
    "            if table_type == 'objectTable':\n",
    "                print(f\"Successfully loaded {df['tract'].nunique()} of {len(files)} CSV files into a single DataFrame.\")\n",
    "            elif table_type == 'objectTable':\n",
    "                print(f\"Successfully loaded {df['visit'].nunique()} of {len(files)} CSV files into a single DataFrame.\")\n",
    "            else:\n",
    "                print(\"Successfully loaded the CSV files into a single DataFrame.\")\n",
    "            print(f\"Total rows in combined DataFrame: {len(df)}\")\n",
    "            #print(df['tract'].value_counts()) # Show the count of rows for each tract\n",
    "    else:\n",
    "        df = pd.DataFrame() # Create an empty DataFrame if no files were found or loaded\n",
    "        if verbose > 0:\n",
    "            print(\"No CSV files found or loaded to create a DataFrame.\")\n",
    "\n",
    "\n",
    "    # Check if 'Unnamed: 0' column exists and drop it\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "        if verbose > 1:\n",
    "            print(\"Removed 'Unnamed: 0' column from the combined DataFrame.\")\n",
    "\n",
    "    if verbose > 1:\n",
    "        display(df.head()) # Display the first few rows of the final DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291a96b-1451-4db1-b43e-445162126a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2.1.3.  Create df_piv using pivot_table\n",
    "\n",
    "# Per Gemini+2.5-Flash:\n",
    "\n",
    "if useCSV:\n",
    "\n",
    "    # Define the columns that will become values in the new DataFrame\n",
    "    value_cols = [\n",
    "        'percent_01', 'percent_50', 'percent_99',\n",
    "        'total_rows', 'nan_count', 'nan_fraction'\n",
    "    ]\n",
    "\n",
    "    # Use pivot_table to reshape the DataFrame\n",
    "    if table_type == 'objectTable':\n",
    "        df_piv = df.pivot_table(\n",
    "            index='tract',          # The column that will become the new index (one row per tract)\n",
    "            columns='colname',      # The column whose unique values will become new columns\n",
    "            values=value_cols       # The columns whose values will populate the new pivoted columns\n",
    "        )\n",
    "    elif table_type == 'sourceTable':\n",
    "        df_piv = df.pivot_table(\n",
    "            index='visit',          # The column that will become the new index (one row per tract)\n",
    "            columns='colname',      # The column whose unique values will become new columns\n",
    "            values=value_cols       # The columns whose values will populate the new pivoted columns\n",
    "        )\n",
    "    else:\n",
    "        print(\"\"\"%s is unrecognized...  Stopping here!\"\"\" % (table_type))\n",
    "        raise StopExecution\n",
    "\n",
    "    # After pivoting, the columns will be a MultiIndex (e.g., ('percent_01', 'ra')).\n",
    "    # We need to flatten them to the desired format: <colname>_<metric>\n",
    "    # The order is (metric, colname), so we need to swap levels and then join.\n",
    "\n",
    "    # Swap the levels of the MultiIndex columns so 'colname' comes first\n",
    "    df_piv.columns = df_piv.columns.swaplevel(0, 1)\n",
    "\n",
    "    # Sort the columns for better readability (optional, but good practice)\n",
    "    df_piv = df_piv.sort_index(axis=1)\n",
    "\n",
    "    # Flatten the MultiIndex columns to a single level as '<colname>_<metric>'\n",
    "    # This creates a new list of column names like 'ra_percent_01', 'ra_percent_50', etc.\n",
    "    df_piv.columns = ['_'.join(col).strip() for col in df_piv.columns.values]\n",
    "\n",
    "    # Reset the index to turn 'tract' from an index back into a regular column (optional)\n",
    "    df_piv = df_piv.reset_index()\n",
    "\n",
    "    # Rename df_piv as res\n",
    "    res = df_piv\n",
    "\n",
    "    if verbose > 1:\n",
    "        print(\"\\n--- Pivoted DataFrame (res(=df_piv)) ---\")\n",
    "        display(res.head())\n",
    "        #print(\"\\nNew res.columns:\", res.columns.tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de234fa-b576-4a08-8062-0395e4de47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Retrieve data from EFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8b954-84ff-40df-8f0c-abe0c40e7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 2.2.1. Prepare query\n",
    "\n",
    "if not useCSV:\n",
    "    query = '''SELECT * FROM \"%s\" WHERE  time > '%s' and time < '%s' ''' % (table_name, time_lo, time_hi)\n",
    "    if verbose > 1:\n",
    "        print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbde54-198b-4305-8716-3996810ab1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.2.2 Connect to EFD database and run query\n",
    "\n",
    "if not useCSV:\n",
    "\n",
    "    efd_client = EfdClient(efd_name, db_name=db_name)\n",
    "    res = await efd_client.influx_client.query(query)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Number of entries returned: ', len(res)) \n",
    "        if table_type == 'objectTable':\n",
    "            print('Unique tracts returned:  ', len(res['tract'].unique()))\n",
    "        elif table_type == 'sourceTable':\n",
    "            print('Unique visits returned:  ', len(res['visit'].unique()))\n",
    "        else:\n",
    "            print(\"\"\"%s is unrecognized...  Stopping here!\"\"\" % (table_type))\n",
    "            raise StopExecution\n",
    "        print('')\n",
    "\n",
    "    if verbose > 1:\n",
    "        display(res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e1348-9d07-4552-aae4-8b88c3d0f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.1 Get a list of column names that contain numeric data\n",
    "numeric_columns = res.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Display the numeric column names\n",
    "if verbose > 2:\n",
    "    print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f157388-3f75-4e83-ad2f-3648c3d9f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.2 Get a list of numeric columns with the suffix \"_nan_fraction\"\n",
    "nan_fraction_columns = [\n",
    "    col for col in res.select_dtypes(include='number').columns if col.endswith('_nan_fraction')\n",
    "]\n",
    "\n",
    "if table_type == 'objectTable':\n",
    "    # Append 'tract' to the front of the list\n",
    "    nan_fraction_columns = ['tract'] + nan_fraction_columns\n",
    "elif table_type == 'sourceTable':\n",
    "    # Append 'visit' to the front of the list\n",
    "    nan_fraction_columns = ['visit'] + nan_fraction_columns\n",
    "else:\n",
    "    print(\"\"\"%s is unrecognized...  Stopping here!\"\"\" % (table_type))\n",
    "    raise StopExecution\n",
    "\n",
    "# Display the filtered column names\n",
    "if verbose > 2:\n",
    "    print(nan_fraction_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba366891-0d91-4357-b794-802c6ed825d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.3 Create a new DataFrame with the selected columns\n",
    "res_nan_fraction = res[nan_fraction_columns]\n",
    "\n",
    "# Display the new DataFrame\n",
    "if verbose > 0:\n",
    "    display(res_nan_fraction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c37d5-c52d-47c2-9422-30bb6e2b01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.4 Plot histograms of the 'NaN fraction in a Tract (or Visit)' \n",
    "## for all columns of res_nan_fraction that contain at least one tract (or visit) \n",
    "## with a NaN fraction > 0.01 for that column\n",
    "\n",
    "df = res_nan_fraction.copy()\n",
    "\n",
    "# Grab relevant tract, marker, and color dictionaries appropriate for the instrument\n",
    "tract_dict, marker_dict, color_dict = instrument_create_tract_markup_color_dict(instrument)\n",
    "\n",
    "# Create the 'fieldname' column using map and fillna\n",
    "if table_type == 'objectTable':\n",
    "    df['fieldname'] = df['tract'].astype(int).map(tract_dict).fillna('Other')\n",
    "else:\n",
    "    df['fieldname'] = 'Other'\n",
    "\n",
    "# Reorder columns so 'fieldname' is second\n",
    "cols = df.columns.tolist()\n",
    "cols.insert(1, cols.pop(cols.index('fieldname')))\n",
    "df = df[cols]\n",
    "\n",
    "# Define filter list\n",
    "if table_type == 'objectTable':\n",
    "    filternameList = ['u', 'g', 'r', 'i', 'z', 'y', 'none']\n",
    "    #filternameList = ['g']   # <-- for testing a single filter\n",
    "else:\n",
    "    filternameList = ['all']\n",
    "\n",
    "# Get numeric columns\n",
    "df_numeric = df.select_dtypes(include='number')\n",
    "\n",
    "# Filter for columns with values > 0.01\n",
    "cols_above_threshold = df_numeric.columns[(df_numeric > 0.01).any()]\n",
    "\n",
    "# Container for summary stats\n",
    "summary_records = []\n",
    "\n",
    "# Plot Preview flag\n",
    "plot_preview = True\n",
    "\n",
    "# Loop over filters...\n",
    "for filtername in filternameList:\n",
    "    print(f\"\\nProcessing filter: {filtername}\")\n",
    "\n",
    "    if filtername in ('none', 'all'):\n",
    "        numeric_cols = cols_above_threshold[~cols_above_threshold.str.match(r'^[ugrizy]_')]\n",
    "    else:\n",
    "        numeric_cols = cols_above_threshold[cols_above_threshold.str.match(f'^{filtername}_')]\n",
    "\n",
    "    # --- CLEANING STEP ---\n",
    "    ignored_special_cols = []\n",
    "    dropped_tracts = []\n",
    "    if table_type == 'objectTable' and filtername != 'all':\n",
    "        filter_cols = list(numeric_cols)\n",
    "\n",
    "        # Define the special columns to ignore\n",
    "        special_cols = [\n",
    "            f\"{filtername}_psfModel_TwoGaussian_n_iter\",\n",
    "            f\"{filtername}_inputCount\"\n",
    "        ]\n",
    "        for sc in special_cols:\n",
    "            if sc in filter_cols:\n",
    "                filter_cols.remove(sc)\n",
    "                ignored_special_cols.append(sc)\n",
    "\n",
    "        if filter_cols:\n",
    "            # Boolean mask: True if all filter_cols are NaN for that row\n",
    "            mask_all_nan = df[filter_cols].isna().all(axis=1)\n",
    "            dropped_count = mask_all_nan.sum()\n",
    "            # Collect the tract IDs that were dropped\n",
    "            dropped_tracts = df.loc[mask_all_nan, 'tract'].tolist()\n",
    "            df_subset = df.loc[~mask_all_nan].copy()\n",
    "            msg = f\"  Dropped {dropped_count} tracts with all-NaN values for filter '{filtername}'\"\n",
    "            if ignored_special_cols:\n",
    "                msg += f\" (ignoring {', '.join(ignored_special_cols)})\"\n",
    "            if dropped_tracts:\n",
    "                msg += f\"\\n    Tracts dropped: {dropped_tracts}\"\n",
    "            print(msg)\n",
    "        else:\n",
    "            df_subset = df.copy()\n",
    "            dropped_count = 0\n",
    "            dropped_tracts = []\n",
    "            print(f\"  No numeric columns (other than {', '.join(special_cols)}) found for filter '{filtername}'\")\n",
    "    else:\n",
    "        df_subset = df.copy()\n",
    "        dropped_count = 0\n",
    "    # ---------------------\n",
    "\n",
    "    # Compute medians for the candidate columns (on the cleaned subset)\n",
    "    if len(numeric_cols) > 0:\n",
    "        #median_vals = df_subset[numeric_cols].median()\n",
    "        # TEMPORARY KLUDGE!!!:  Mask out values >= 1.0 so they don't affect the median\n",
    "        median_vals = df_subset[numeric_cols].mask(df_subset[numeric_cols] >= 1.0).median()\n",
    "        # Sort descending by median\n",
    "        numeric_cols = median_vals.sort_values(ascending=False).index.tolist()\n",
    "    else:\n",
    "        print(f\"  Skipping filter '{filtername}' (no numeric columns after cleaning)\")\n",
    "        summary_records.append({\n",
    "            \"Filter\": filtername,\n",
    "            \"Columns Plotted\": 0,\n",
    "            \"Tracts Dropped\": dropped_count,\n",
    "            \"Dropped Tracts\": dropped_tracts,\n",
    "            \"Ignored Special Cols\": ignored_special_cols\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Build super title and output filename\n",
    "    if table_type == 'objectTable':\n",
    "        superTitle = (\n",
    "            \"'NaN Fraction in Tract' for ObjectTable Columns \\n\"\n",
    "            \"(with at least one Tract with a NaN Fraction of at least 0.01) \\n\"\n",
    "            f\"Collection: {collectionName}\"\n",
    "        )\n",
    "        outputPlotFile = f\"NaN_Fraction_in_Tract_histograms_{filtername}.png\"\n",
    "    elif table_type == 'sourceTable':\n",
    "        superTitle = (\n",
    "            \"'NaN Fraction in Visit' for SourceTable Columns \\n\"\n",
    "            \"(with at least one Visit with a NaN Fraction of at least 0.01) \\n\"\n",
    "            f\"Collection: {collectionName}\"\n",
    "        )\n",
    "        outputPlotFile = f\"NaN_Fraction_in_Visit_histograms_{filtername}.png\"\n",
    "    else:\n",
    "        superTitle = ''\n",
    "        outputPlotFile = f\"NaN_Fraction_histograms_{filtername}.png\"\n",
    "\n",
    "    # Full plot (save to file)\n",
    "    plot_hist_and_scatter(\n",
    "        df=df_subset,\n",
    "        numeric_cols=numeric_cols,\n",
    "        filtername=filtername,\n",
    "        table_type=table_type,\n",
    "        collectionName=collectionName,\n",
    "        marker_dict=marker_dict,\n",
    "        color_dict=color_dict,\n",
    "        super_title=superTitle,\n",
    "        output_file=outputPlotFile,\n",
    "        show_inline=False\n",
    "    )\n",
    "    print(f\"  Saved big plot to: {os.path.abspath(outputPlotFile)}\")\n",
    "\n",
    "\n",
    "    # Only plot preview for the first (or only) filtername...\n",
    "    if plot_preview:\n",
    "        # Preview: first 20 numeric columns (show inline)\n",
    "        plot_hist_and_scatter(\n",
    "            df=df_subset,\n",
    "            numeric_cols=numeric_cols[:20],\n",
    "            filtername=filtername,\n",
    "            table_type=table_type,\n",
    "            collectionName=collectionName,\n",
    "            marker_dict=marker_dict,\n",
    "            color_dict=color_dict,\n",
    "            super_title=f\"PREVIEW (First 20 Numeric Columns, descending order of median NaN_Fraction, filter={filtername}):\\n{superTitle}\\nFull plot in {os.path.abspath(outputPlotFile)}\",\n",
    "            output_file=None,\n",
    "            show_inline=True\n",
    "        )\n",
    "        plot_preview = False\n",
    "\n",
    "    # Record summary info\n",
    "    summary_records.append({\n",
    "        \"Filter\": filtername,\n",
    "        \"Columns Plotted\": len(numeric_cols),\n",
    "        \"Tracts Dropped\": dropped_count,\n",
    "        \"Dropped Tracts\": dropped_tracts,\n",
    "        \"Ignored Special Cols\": ignored_special_cols\n",
    "    })\n",
    "\n",
    "\n",
    "# --- SUMMARY TABLE ---\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "print(\"\\n=== Summary of Filters Processed ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b90432-eaa4-4225-a995-876a3ac7d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.5 Create a dataframe, summary_res_nan_fraction, that contains a summary description of res_nan_fraction\n",
    "## (ignoring tract+filter combinations that are all-NaN, except for the special *_psfModel_TwoGaussian_n_iter column)\n",
    "\n",
    "df_clean = res_nan_fraction.copy()\n",
    "\n",
    "if table_type == 'objectTable':\n",
    "\n",
    "    # Loop over filters\n",
    "    for f in ['u','g','r','i','z','y']:\n",
    "        filter_cols = [c for c in df_clean.columns if c.startswith(f\"{f}_\")]\n",
    "        special_cols = [f\"{f}_psfModel_TwoGaussian_n_iter\", f\"{f}_inputColumn\"]\n",
    "        for sc in special_cols:\n",
    "            if sc in filter_cols:\n",
    "                filter_cols.remove(sc)\n",
    "        if filter_cols:\n",
    "            mask_all_nan = df_clean[filter_cols].isna().all(axis=1)\n",
    "            df_clean = df_clean.loc[~mask_all_nan]\n",
    "\n",
    "    # TEMPORARY KLUDGE!!!: remove tract+column combos with NaN_Fraction >= 1.0\n",
    "    df_clean = df_clean.mask(df_clean >= 1.0)\n",
    "\n",
    "    # Now build summary, dropping tract column if present\n",
    "    if 'tract' in df_clean.columns:\n",
    "        summary_res_nan_fraction = df_clean.drop(columns=['tract']).describe(percentiles=[0.01,0.99]).T\n",
    "    else:\n",
    "        summary_res_nan_fraction = df_clean.describe(percentiles=[0.01,0.99]).T\n",
    "\n",
    "elif table_type == 'sourceTable':\n",
    "\n",
    "    # TEMPORARY KLUDGE!!!: remove tract+column combos with NaN_Fraction >= 1.0\n",
    "    df_clean = df_clean.mask(df_clean >= 1.0)\n",
    "\n",
    "    # Now build summary, dropping visit column if present\n",
    "    if 'visit' in res_nan_fraction.columns:\n",
    "        summary_res_nan_fraction = df_clean.drop(columns=['visit']).describe(percentiles=[0.01,0.99]).T\n",
    "    else:\n",
    "        summary_res_nan_fraction = df_clean.describe(percentiles=[0.01,0.99]).T\n",
    "\n",
    "if verbose > 2:\n",
    "    display(summary_res_nan_fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb1b7b-2133-4b61-b1e3-79efa6c86655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 3.6 Plot the summary dataframe (split by filter only for objectTable)\n",
    "## (ignoring tract+filter combinations that are all-NaN, except for the special *_psfModel_TwoGaussian_n_iter and 8_inputCountscolumns)\n",
    "\n",
    "# Prepare DataFrame\n",
    "df = summary_res_nan_fraction.copy()\n",
    "df['name'] = df.index.str.replace('_nan_fraction', '', regex=False)\n",
    "\n",
    "# Define filter groups only if objectTable\n",
    "if table_type == 'objectTable':\n",
    "    filternames = ['u', 'g', 'r', 'i', 'z', 'y', 'none']\n",
    "    #filternames = ['g']   # <-- for testing a single filter\n",
    "else:\n",
    "    filternames = ['all']\n",
    "\n",
    "# Plot Preview flag\n",
    "plot_preview = True\n",
    "\n",
    "# Loop over filternames...\n",
    "for f in filternames:\n",
    "    if f in ('none','all'):\n",
    "        mask = ~df['name'].str.match(r'^[ugrizy]_')\n",
    "    else:\n",
    "        mask = df['name'].str.startswith(f\"{f}_\")\n",
    "\n",
    "    df_subset = df.loc[mask].copy()\n",
    "    if df_subset.empty:\n",
    "        continue\n",
    "\n",
    "    # --- CLEANING STEP drop all-NaN tract+filter combos (ignoring special cols)---\n",
    "    if table_type == 'objectTable' and f not in ('none','all'):\n",
    "        filter_cols = [c for c in res_nan_fraction.columns if c.startswith(f\"{f}_\")]\n",
    "        special_cols = [f\"{f}_psfModel_TwoGaussian_n_iter\", f\"{f}_inputCount\"]\n",
    "        for sc in special_cols:\n",
    "            if sc in filter_cols:\n",
    "                filter_cols.remove(sc)\n",
    "        if filter_cols:\n",
    "            mask_all_nan = res_nan_fraction[filter_cols].isna().all(axis=1)\n",
    "            valid_tracts = res_nan_fraction.loc[~mask_all_nan, 'tract']\n",
    "            # Keep only rows in df_subset corresponding to valid tracts\n",
    "            # (summary_res_nan_fraction is column-oriented, so this step ensures\n",
    "            # we only keep columns relevant to valid tracts)\n",
    "            # If you only need to drop filters with no valid tracts, you can skip this\n",
    "            df_subset = df_subset.loc[df_subset.index.isin([col for col in df_subset.index if any(col.startswith(f\"{f}_\") for f in [f])])]    \n",
    "    \n",
    "    if df_subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Build output filename\n",
    "    if table_type == 'objectTable':\n",
    "        outputPlotFile = f'NaN_Fraction_in_Tract_{f}_1percent_50percent_99percent.png'\n",
    "        superTitle = (\n",
    "            f\"'NaN Fraction in Tract' for ObjectTable Columns (filter={f})\\n\"\n",
    "            f\"Collection: {collectionName}\"\n",
    "        )\n",
    "    elif table_type == 'sourceTable':\n",
    "        outputPlotFile = 'NaN_Fraction_in_Visit_1percent_50percent_99percent.png'\n",
    "        superTitle = (\n",
    "            \"'NaN Fraction in Visit' for SourceTable Columns\\n\"\n",
    "            f\"Collection: {collectionName}\"\n",
    "        )\n",
    "    else:\n",
    "        outputPlotFile = 'NaN_Fraction_1percent_50percent_99percent.png'\n",
    "        superTitle = f\"'NaN Fraction' Columns\\nCollection: {collectionName}\"\n",
    "\n",
    "    #Full plot (save to file)\n",
    "    plot_nan_fraction_summary(\n",
    "        df=df_subset,\n",
    "        table_type=table_type,\n",
    "        collectionName=collectionName,\n",
    "        output_file=outputPlotFile,\n",
    "        show_inline=False,\n",
    "        title_prefix=\"\",\n",
    "        max_height_per_row=0.25,\n",
    "        base_height=4\n",
    "    )\n",
    "    print(f\"Saved Summary plot for filter '{f}' to: {os.path.abspath(outputPlotFile)}\")\n",
    "\n",
    "    # Only plot preview for the first (or only) filtername...\n",
    "    if plot_preview:    \n",
    "        # Preview: always top 20 rows of this subset inline\n",
    "        plot_nan_fraction_summary(\n",
    "            df=df_subset.head(20),\n",
    "            table_type=table_type,\n",
    "            collectionName=collectionName,\n",
    "            output_file=None,\n",
    "            show_inline=True,\n",
    "            title_prefix=f\"PREVIEW (First 20 Numeric Columns, alphabetical order, filter={f}):\",\n",
    "            max_height_per_row=0.4,\n",
    "            base_height=4,\n",
    "            full_plot_path=outputPlotFile\n",
    "        )\n",
    "        plot_preview = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41cc179-d074-4e57-84f4-10d369770fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 4.0 Clean up\n",
    "\n",
    "#Reset pandas to its default maximum rows to print to screen\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f99f13-2db3-4226-95b7-e80ab140f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise StopExecution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8be388-5610-4681-8359-b7fb025137f8",
   "metadata": {},
   "source": [
    "## SandBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaf18b-c839-4bff-bcca-228bc55d47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
